import requests
from requests.auth import HTTPBasicAuth
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle
import json
from datetime import datetime
import time
import csv
import os

class ICDMmsDataProcessor:
    """L·ªõp x·ª≠ l√Ω d·ªØ li·ªáu ICD-11 MMS Linearization cho RAG Chatbot ch·∫©n ƒëo√°n"""
    
    def __init__(self, client_id, client_secret):
        self.client_id = client_id
        self.client_secret = client_secret
        self.token_url = "https://icdaccessmanagement.who.int/connect/token"
        self.icd_api_base = "https://id.who.int/icd"
        self.access_token = None
        self.token_expires_at = None
        
    def get_access_token(self):
        """L·∫•y access token t·ª´ WHO ICD API"""
        try:
            print("üîê ƒêang l·∫•y access token t·ª´ WHO ICD API...")
            
            data = {
                "grant_type": "client_credentials",
                "scope": "icdapi_access"
            }
            
            response = requests.post(
                self.token_url, 
                data=data, 
                auth=HTTPBasicAuth(self.client_id, self.client_secret),
                timeout=30
            )
            response.raise_for_status()
            
            token_data = response.json()
            self.access_token = token_data["access_token"]
            # Token th∆∞·ªùng c√≥ hi·ªáu l·ª±c 1 gi·ªù
            self.token_expires_at = datetime.now().timestamp() + token_data.get("expires_in", 3600)
            
            print("‚úÖ ƒê√£ l·∫•y access token th√†nh c√¥ng")
            return True
            
        except Exception as e:
            print(f"‚ùå L·ªói khi l·∫•y access token: {e}")
            return False
    
    def is_token_valid(self):
        """Ki·ªÉm tra token c√≥ c√≤n hi·ªáu l·ª±c kh√¥ng"""
        if not self.access_token:
            return False
        return datetime.now().timestamp() < (self.token_expires_at - 300)  # Refresh 5 ph√∫t tr∆∞·ªõc khi h·∫øt h·∫°n
    
    def get_headers(self):
        """L·∫•y headers cho API request"""
        if not self.is_token_valid():
            self.get_access_token()
            
        return {
            "Authorization": f"Bearer {self.access_token}",
            "Accept": "application/json",
            "API-Version": "v2",
            "Accept-Language": "en"
        }
    
    def get_icd_entity(self, entity_uri, retries=3):
        """L·∫•y th√¥ng tin entity t·ª´ ICD API v·ªõi retry logic"""
        headers = self.get_headers()
        
        for attempt in range(retries):
            try:
                response = requests.get(entity_uri, headers=headers, verify=False, timeout=15)
                response.raise_for_status()
                return response.json()
                
            except requests.exceptions.RequestException as e:
                print(f"‚ùå L·ªói attempt {attempt+1} khi g·ªçi {entity_uri}: {e}")
                if attempt < retries - 1:
                    time.sleep(1 * (attempt + 1))
                else:
                    print(f"‚ùå Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu t·ª´ {entity_uri} sau {retries} l·∫ßn th·ª≠")
        
        return None
    
    def extract_medical_text_for_rag(self, entity_data, entity_uri=""):
        """Tr√≠ch xu·∫•t CH·ªà nh·ªØng tr∆∞·ªùng QUAN TR·ªåNG cho RAG chatbot ch·∫©n ƒëo√°n b·ªánh"""
        text_parts = []
        
        try:
            # 1. TITLE - T√™n b·ªánh (B·∫ÆT BU·ªòC)
            title = entity_data.get('title', {})
            if isinstance(title, dict) and '@value' in title:
                text_parts.append(f"T√™n b·ªánh: {title['@value']}")
            
            # 2. CODE - M√£ ICD-11 (QUAN TR·ªåNG NH·∫§T)
            code = entity_data.get('code')
            if code:
                text_parts.append(f"M√£ ICD: {code}")
            
            # 3. DEFINITION - ƒê·ªãnh nghƒ©a b·ªánh (QUAN TR·ªåNG)
            definition = entity_data.get('definition', {})
            if isinstance(definition, dict) and '@value' in definition:
                text_parts.append(f"ƒê·ªãnh nghƒ©a: {definition['@value']}")
            
            # 4. DIAGNOSTIC CRITERIA - Ti√™u ch√≠ ch·∫©n ƒëo√°n (R·∫§T QUAN TR·ªåNG cho AI)
            diagnostic_criteria = entity_data.get('diagnosticCriteria', {})
            if isinstance(diagnostic_criteria, dict) and '@value' in diagnostic_criteria:
                text_parts.append(f"Ti√™u ch√≠ ch·∫©n ƒëo√°n: {diagnostic_criteria['@value']}")
            
            # 5. INDEX TERMS - Thu·∫≠t ng·ªØ t√¨m ki·∫øm (QUAN TR·ªåNG cho RAG search)
            index_terms = entity_data.get('indexTerm', [])
            if index_terms:
                terms = []
                for term in index_terms[:5]:  # Ch·ªâ l·∫•y 5 terms ƒë·∫ßu
                    if isinstance(term, dict):
                        label = term.get('label', {})
                        if isinstance(label, dict) and '@value' in label:
                            terms.append(label['@value'])
                
                if terms:
                    text_parts.append(f"Thu·∫≠t ng·ªØ li√™n quan: {', '.join(terms)}")
            
            # 6. INCLUSION - Bao g·ªìm (Gi√∫p AI hi·ªÉu r√µ h∆°n)
            inclusions = entity_data.get('inclusion', [])
            if inclusions:
                inc_terms = []
                for inc in inclusions[:3]:  # Ch·ªâ l·∫•y 3 inclusion ƒë·∫ßu
                    if isinstance(inc, dict):
                        label = inc.get('label', {})
                        if isinstance(label, dict) and '@value' in label:
                            inc_terms.append(label['@value'])
                
                if inc_terms:
                    text_parts.append(f"Bao g·ªìm: {', '.join(inc_terms)}")
            
            # 7. EXCLUSION - Lo·∫°i tr·ª´ (Gi√∫p AI ph√¢n bi·ªát)
            exclusions = entity_data.get('exclusion', [])
            if exclusions:
                exc_terms = []
                for exc in exclusions[:3]:  # Ch·ªâ l·∫•y 3 exclusion ƒë·∫ßu
                    if isinstance(exc, dict):
                        label = exc.get('label', {})
                        if isinstance(label, dict) and '@value' in label:
                            exc_terms.append(label['@value'])
                
                if exc_terms:
                    text_parts.append(f"Lo·∫°i tr·ª´: {', '.join(exc_terms)}")
            
            # Th√™m Browser URL ƒë·ªÉ truy c·∫≠p tr·ª±c ti·∫øp
            browser_url = entity_data.get('browserUrl')
            if browser_url:
                text_parts.append(f"üîó Xem chi ti·∫øt: {browser_url}")
            
            # Ch·ªâ tr·∫£ v·ªÅ n·∫øu c√≥ √≠t nh·∫•t title v√† m·ªôt tr∆∞·ªùng quan tr·ªçng kh√°c
            if len(text_parts) >= 2:
                return "\n".join(text_parts)
            else:
                return None  # Skip entities kh√¥ng ƒë·ªß th√¥ng tin
            
        except Exception as e:
            print(f"‚ùå L·ªói khi tr√≠ch xu·∫•t: {e}")
            return None
    
    def create_rag_chunks_with_metadata(self, entity_data, entity_uri, text_splitter):
        """T·∫°o chunks v·ªõi metadata chi ti·∫øt cho RAG system"""
        chunks_with_metadata = []
        
        try:
            # Tr√≠ch xu·∫•t metadata c∆° b·∫£n
            title = entity_data.get('title', {})
            entity_name = title.get('@value', 'Unknown') if isinstance(title, dict) else str(title)
            
            # L·∫•y code t·ª´ URI n·∫øu c√≥
            entity_code = entity_uri.split('/')[-1] if entity_uri else None
            
            # Tr√≠ch xu·∫•t text cho RAG
            entity_text = self.extract_medical_text_for_rag(entity_data, entity_uri)
            
            # Chia th√†nh chunks
            text_chunks = text_splitter.split_text(entity_text)
            
            for i, chunk in enumerate(text_chunks):
                chunk_metadata = {
                    'source_type': 'icd_entity',
                    'entity_name': entity_name,
                    'entity_uri': entity_uri,
                    'entity_code': entity_code,
                    'data_type': 'medical_diagnosis',
                    'chunk_index': i,
                    'chunk_size': len(chunk),
                    'total_chunks': len(text_chunks),
                    'fetch_timestamp': datetime.now().isoformat(),
                    'entity_type': 'medical_condition',
                    'chunk_type': 'diagnosis_info',
                    # Th√¥ng tin ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu cho RAG
                    'has_definition': bool(entity_data.get('definition', {}).get('@value')),
                    'has_diagnostic_criteria': bool(entity_data.get('diagnosticCriteria', {}).get('@value')),
                    'has_icd_code': bool(entity_data.get('code')),
                    'has_index_terms': bool(entity_data.get('indexTerm')),
                    'has_inclusions': bool(entity_data.get('inclusion')),
                    'has_exclusions': bool(entity_data.get('exclusion')),
                    'browser_url': entity_data.get('browserUrl', ''),
                    'quality_score': self.calculate_quality_score(entity_data)
                }
                
                chunks_with_metadata.append({
                    'text': chunk,
                    'metadata': chunk_metadata
                })
            
            return chunks_with_metadata
            
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫°o RAG chunks: {e}")
            return []
    
    def calculate_quality_score(self, entity_data):
        """T√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu cho RAG (0-100)"""
        score = 0
        
        # Title (b·∫Øt bu·ªôc) - 20 ƒëi·ªÉm
        if entity_data.get('title', {}).get('@value'):
            score += 20
        
        # ICD Code (r·∫•t quan tr·ªçng) - 25 ƒëi·ªÉm
        if entity_data.get('code'):
            score += 25
        
        # Definition (quan tr·ªçng) - 20 ƒëi·ªÉm
        if entity_data.get('definition', {}).get('@value'):
            score += 20
        
        # Diagnostic Criteria (r·∫•t quan tr·ªçng cho AI) - 25 ƒëi·ªÉm
        if entity_data.get('diagnosticCriteria', {}).get('@value'):
            score += 25
        
        # Index Terms (h·ªØu √≠ch cho t√¨m ki·∫øm) - 5 ƒëi·ªÉm
        if entity_data.get('indexTerm'):
            score += 5
        
        # Inclusion/Exclusion (h·ªØu √≠ch) - 3 ƒëi·ªÉm
        if entity_data.get('inclusion') or entity_data.get('exclusion'):
            score += 3
        
        # Browser URL (ti·ªán √≠ch) - 2 ƒëi·ªÉm
        if entity_data.get('browserUrl'):
            score += 2
        
        return min(score, 100)  # Gi·ªõi h·∫°n 100 ƒëi·ªÉm
    
    def load_existing_entities(self, filename="mms_entities_progress.pkl"):
        """Load danh s√°ch entities ƒë√£ l·∫•y t·ª´ file progress"""
        try:
            # ƒê·∫£m b·∫£o t√¨m file trong th∆∞ m·ª•c chatbox/data
            if not filename.startswith('chatbox/') and not os.path.isabs(filename):
                filename = os.path.join(os.path.dirname(__file__), 'data', filename)
                
            if os.path.exists(filename):
                with open(filename, "rb") as f:
                    progress_data = pickle.load(f)
                    return progress_data.get('seen_uris', set()), progress_data.get('all_entities', [])
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load progress file: {e}")
        return set(), []
    
    def get_next_batch_number(self):
        """T√≠nh s·ªë batch ti·∫øp theo d·ª±a tr√™n s·ªë batch files ƒë√£ c√≥"""
        current_dir = os.path.join(os.path.dirname(__file__), 'data')
        batch_number = 1
        
        while True:
            batch_filename = f"mms_batch_{batch_number:03d}.pkl"
            batch_filepath = os.path.join(current_dir, batch_filename)
            if os.path.exists(batch_filepath):
                batch_number += 1
            else:
                break
        
        return batch_number
    
    def save_progress(self, seen_uris, all_entities, filename="mms_entities_progress.pkl"):
        """L∆∞u progress sau m·ªói batch"""
        try:
            # ƒê·∫£m b·∫£o file ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c chatbox/data
            if not filename.startswith('chatbox/') and not os.path.isabs(filename):
                filename = os.path.join(os.path.dirname(__file__), 'data', filename)
                
            progress_data = {
                'seen_uris': seen_uris,
                'all_entities': all_entities,
                'last_update': datetime.now().isoformat(),
                'total_entities': len(all_entities)
            }
            with open(filename, "wb") as f:
                pickle.dump(progress_data, f)
            print(f"üíæ ƒê√£ l∆∞u progress: {len(all_entities)} entities")
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u progress: {e}")
    
    def is_token_expired(self, response):
        """Ki·ªÉm tra token c√≥ h·∫øt h·∫°n kh√¥ng"""
        if response.status_code == 401:
            return True
        return False
    
    def get_headers_with_refresh(self):
        """L·∫•y headers v√† t·ª± ƒë·ªông refresh token n·∫øu h·∫øt h·∫°n"""
        headers = self.get_headers()
        return headers
    
    def get_icd_entity_with_retry(self, entity_uri, retries=3):
        """L·∫•y th√¥ng tin entity v·ªõi retry v√† token refresh"""
        for attempt in range(retries):
            try:
                headers = self.get_headers_with_refresh()
                response = requests.get(entity_uri, headers=headers, verify=False, timeout=15)
                
                # N·∫øu token h·∫øt h·∫°n, refresh v√† th·ª≠ l·∫°i
                if self.is_token_expired(response):
                    print(f"‚ö†Ô∏è Token h·∫øt h·∫°n, ƒëang refresh...")
                    if self.get_access_token():
                        headers = self.get_headers_with_refresh()
                        response = requests.get(entity_uri, headers=headers, verify=False, timeout=15)
                
                response.raise_for_status()
                return response.json()
                
            except requests.exceptions.RequestException as e:
                print(f"‚ùå L·ªói attempt {attempt+1} khi g·ªçi {entity_uri}: {e}")
                if attempt < retries - 1:
                    time.sleep(2 * (attempt + 1))
                else:
                    print(f"‚ùå Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu t·ª´ {entity_uri} sau {retries} l·∫ßn th·ª≠")
        
        return None
    
    def get_mms_releases(self):
        """L·∫•y danh s√°ch t·∫•t c·∫£ c√°c release t·ª´ MMS"""
        try:
            mms_url = f"{self.icd_api_base}/release/11/mms"
            print(f"üîç L·∫•y danh s√°ch releases t·ª´: {mms_url}")
            
            data = self.get_icd_entity_with_retry(mms_url)
            if not data:
                return []
            
            releases = data.get('release', [])
            latest_release = data.get('latestRelease')
            
            print(f"‚úÖ T√¨m th·∫•y {len(releases)} releases")
            print(f"‚úÖ Latest release: {latest_release}")
            
            # S·ª≠ d·ª•ng latest release ho·∫∑c fallback
            if latest_release:
                return [latest_release]
            elif releases:
                return [releases[0]]  # L·∫•y release ƒë·∫ßu ti√™n
            else:
                return []
                
        except Exception as e:
            print(f"‚ùå L·ªói khi l·∫•y MMS releases: {e}")
            return []
    
    def get_release_children(self, release_url):
        """L·∫•y c√°c children t·ª´ m·ªôt release c·ª• th·ªÉ"""
        try:
            print(f"üîç L·∫•y children t·ª´ release: {release_url}")
            
            data = self.get_icd_entity_with_retry(release_url)
            if not data:
                return []
            
            children = data.get('child', [])
            print(f"‚úÖ T√¨m th·∫•y {len(children)} children trong release")
            
            return children
            
        except Exception as e:
            print(f"‚ùå L·ªói khi l·∫•y children t·ª´ release: {e}")
            return []
    
    
    def fetch_entities_for_rag(self, batch_size=100, max_depth=10):
        """L·∫•y TO√ÄN B·ªò entities t·ª´ ICD API v·ªõi batch processing v√† resume capability"""
        print("üåç B·∫Øt ƒë·∫ßu l·∫•y TO√ÄN B·ªò d·ªØ li·ªáu ICD cho RAG (kh√¥ng gi·ªõi h·∫°n)")
        print(f"üì¶ Batch size: {batch_size} entities")
        print(f"üîÑ Max depth: {max_depth}")
        
        # Load progress t·ª´ l·∫ßn ch·∫°y tr∆∞·ªõc (n·∫øu c√≥)
        seen_uris, all_entities = self.load_existing_entities()
        start_count = len(all_entities)
        
        if start_count > 0:
            print(f"üîÑ Resume t·ª´ l·∫ßn ch·∫°y tr∆∞·ªõc: ƒë√£ c√≥ {start_count} entities")
        
        # L·∫•y c√°c release v√† children theo ƒë√∫ng flow API
        releases = self.get_mms_releases()
        if not releases:
            print("‚ùå Kh√¥ng th·ªÉ l·∫•y MMS releases")
            return all_entities
        
        # L·∫•y children t·ª´ t·∫•t c·∫£ releases
        start_uris = []
        for release_url in releases:
            children = self.get_release_children(release_url)
            start_uris.extend(children)
        
        # T·∫°o queue t·ª´ release children ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω
        queue = [(uri, 0) for uri in start_uris if uri not in seen_uris]
        
        # Th√™m t·∫•t c·∫£ children ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω t·ª´ c√°c entities ƒë√£ c√≥
        unprocessed_children_count = 0
        for entity in all_entities:
            entity_data = entity.get('data', {})
            entity_depth = entity.get('depth', 0)
            
            if 'child' in entity_data and entity_depth < max_depth:
                for child_uri in entity_data['child']:
                    if child_uri not in seen_uris:
                        queue.append((child_uri, entity_depth + 1))
                        unprocessed_children_count += 1
        
        print(f"üéØ S·∫Ω b·∫Øt ƒë·∫ßu t·ª´ {len(queue)} URIs:")
        print(f"   - Release children ch∆∞a x·ª≠ l√Ω: {len([uri for uri, depth in queue if depth == 0])}")
        print(f"   - Entity children ch∆∞a x·ª≠ l√Ω: {unprocessed_children_count}")
        
        # T√≠nh batch_count d·ª±a tr√™n s·ªë batch files ƒë√£ c√≥
        batch_count = self.get_next_batch_number()
        entities_in_current_batch = []
        
        while queue:
            current_uri, depth = queue.pop(0)
            
            if current_uri in seen_uris or depth > max_depth:
                continue
                
            print(f"üì° L·∫•y: {current_uri} (depth: {depth}, total: {len(all_entities)})")
            entity_data = self.get_icd_entity_with_retry(current_uri)
            
            if entity_data:
                entity_item = {
                    'uri': current_uri,
                    'data': entity_data,
                    'depth': depth,
                    'timestamp': datetime.now().isoformat()
                }
                
                all_entities.append(entity_item)
                entities_in_current_batch.append(entity_item)
                seen_uris.add(current_uri)
                
                # Th√™m children v√†o queue
                if depth < max_depth and 'child' in entity_data:
                    for child_uri in entity_data['child']:
                        if child_uri not in seen_uris:
                            queue.append((child_uri, depth + 1))
                
                title = entity_data.get('title', {}).get('@value', 'Unknown')
                print(f"‚úÖ ƒê√£ l·∫•y: {title}")
                
                # Ki·ªÉm tra n·∫øu ƒë·ªß batch size th√¨ l∆∞u progress
                if len(entities_in_current_batch) >= batch_size:
                    print(f"\nüíæ L∆∞u batch #{batch_count} ({batch_size} entities)")
                    self.save_progress(seen_uris, all_entities)
                    
                    # T·∫°o embeddings v√† l∆∞u cho batch n√†y
                    self.process_and_save_batch(entities_in_current_batch, batch_count)
                    
                    entities_in_current_batch = []
                    batch_count += 1  # TƒÉng batch_count sau khi l∆∞u
                    print(f"üìä T·ªïng ti·∫øn ƒë·ªô: {len(all_entities)} entities, {len(queue)} c√≤n l·∫°i trong queue\n")
                
                # Delay ƒë·ªÉ tr√°nh rate limiting
                time.sleep(0.3)
            else:
                print(f"‚ùå Kh√¥ng th·ªÉ l·∫•y: {current_uri}")
        
        # X·ª≠ l√Ω batch cu·ªëi c√πng (n·∫øu c√≥)
        if entities_in_current_batch:
            print(f"\nüíæ L∆∞u batch cu·ªëi #{batch_count} ({len(entities_in_current_batch)} entities)")
            self.save_progress(seen_uris, all_entities)
            self.process_and_save_batch(entities_in_current_batch, batch_count)
        
        print(f"\nüéâ HO√ÄN TH√ÄNH! ƒê√£ l·∫•y t·ªïng c·ªông {len(all_entities)} entities")
        print(f"üìà TƒÉng t·ª´ {start_count} l√™n {len(all_entities)} entities")
        
        return all_entities
    
    def process_and_save_batch(self, batch_entities, batch_number):
        """X·ª≠ l√Ω v√† l∆∞u m·ªôt batch entities"""
        try:
            print(f"üîÑ ƒêang x·ª≠ l√Ω batch #{batch_number}...")
            
            # Kh·ªüi t·∫°o text splitter v√† embedder n·∫øu ch∆∞a c√≥
            if not hasattr(self, 'text_splitter'):
                self.text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=500, chunk_overlap=50
                )
            
            if not hasattr(self, 'embedder'):
                try:
                    self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
                except:
                    print("‚ö†Ô∏è Kh√¥ng th·ªÉ load embedder cho batch processing")
                    return
            
            # T·∫°o chunks cho batch n√†y (ch·ªâ l·∫•y entities c√≥ ƒë·ªß th√¥ng tin)
            batch_chunks = []
            valid_entities = 0
            
            for entity_item in batch_entities:
                try:
                    # Ki·ªÉm tra xem entity c√≥ ƒë·ªß th√¥ng tin kh√¥ng
                    entity_text = self.extract_medical_text_for_rag(entity_item['data'], entity_item['uri'])
                    if entity_text:  # Ch·ªâ x·ª≠ l√Ω n·∫øu c√≥ th√¥ng tin h·ªØu √≠ch
                        chunks = self.create_rag_chunks_with_metadata(
                            entity_item['data'], 
                            entity_item['uri'], 
                            self.text_splitter
                        )
                        batch_chunks.extend(chunks)
                        valid_entities += 1
                    else:
                        print(f"‚ö†Ô∏è Skip entity kh√¥ng ƒë·ªß th√¥ng tin: {entity_item['uri']}")
                        
                except Exception as e:
                    print(f"‚ùå L·ªói x·ª≠ l√Ω entity {entity_item['uri']}: {e}")
            
            print(f"‚úÖ X·ª≠ l√Ω {valid_entities}/{len(batch_entities)} entities h·ªØu √≠ch trong batch #{batch_number}")
            
            if batch_chunks:
                # T·∫°o embeddings cho batch
                texts = [chunk['text'] for chunk in batch_chunks]
                embeddings = self.embedder.encode(texts, convert_to_numpy=True)
                
                # L∆∞u batch data
                batch_data = {
                    'batch_number': batch_number,
                    'chunks': batch_chunks,
                    'embeddings': embeddings,
                    'entities_count': len(batch_entities),
                    'chunks_count': len(batch_chunks),
                    'timestamp': datetime.now().isoformat()
                }
                
                # ƒê·∫£m b·∫£o batch file ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c chatbox/data
                batch_filename = f"mms_batch_{batch_number:03d}.pkl"
                batch_filepath = os.path.join(os.path.dirname(__file__), 'data', batch_filename)
                
                with open(batch_filepath, "wb") as f:
                    pickle.dump(batch_data, f)
                
                print(f"‚úÖ ƒê√£ l∆∞u batch #{batch_number}: {len(batch_chunks)} chunks t·ª´ {valid_entities} entities v√†o {batch_filename}")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω batch #{batch_number}: {e}")
    
    def combine_all_batches(self):
        """K·∫øt h·ª£p t·∫•t c·∫£ c√°c batch files th√†nh file cu·ªëi c√πng"""
        print("üîÑ ƒêang k·∫øt h·ª£p t·∫•t c·∫£ c√°c batch files...")
        
        # T√¨m t·∫•t c·∫£ batch files trong th∆∞ m·ª•c chatbox/data
        batch_files = []
        batch_number = 1
        current_dir = os.path.join(os.path.dirname(__file__), 'data')
        
        while True:
            batch_filename = f"mms_batch_{batch_number:03d}.pkl"
            batch_filepath = os.path.join(current_dir, batch_filename)
            if os.path.exists(batch_filepath):
                batch_files.append(batch_filepath)
                batch_number += 1
            else:
                break
        
        if not batch_files:
            print("‚ùå Kh√¥ng t√¨m th·∫•y batch files n√†o")
            return None
        
        print(f"üìÅ T√¨m th·∫•y {len(batch_files)} batch files")
        
        # K·∫øt h·ª£p t·∫•t c·∫£ chunks v√† embeddings
        all_chunks = []
        all_embeddings = []
        total_entities = 0
        
        for batch_file in batch_files:
            try:
                print(f"üìñ ƒêang ƒë·ªçc {batch_file}...")
                with open(batch_file, "rb") as f:
                    batch_data = pickle.load(f)
                
                all_chunks.extend(batch_data['chunks'])
                all_embeddings.append(batch_data['embeddings'])
                total_entities += batch_data['entities_count']
                
                print(f"‚úÖ ƒê√£ ƒë·ªçc {batch_data['chunks_count']} chunks t·ª´ {batch_file}")
                
            except Exception as e:
                print(f"‚ùå L·ªói khi ƒë·ªçc {batch_file}: {e}")
                continue
        
        if not all_chunks:
            print("‚ùå Kh√¥ng c√≥ chunks n√†o ƒë·ªÉ k·∫øt h·ª£p")
            return None
        
        # K·∫øt h·ª£p embeddings
        print("üîÑ ƒêang k·∫øt h·ª£p embeddings...")
        combined_embeddings = np.vstack(all_embeddings)
        
        # T·∫°o metadata list
        texts = [chunk['text'] for chunk in all_chunks]
        metadata_list = [chunk['metadata'] for chunk in all_chunks]
        
        # T·∫°o final chunks data
        final_chunks_data = {
            'chunks': all_chunks,
            'texts': texts,
            'metadata': metadata_list,
            'created_at': datetime.now().isoformat(),
            'total_chunks': len(all_chunks),
            'total_entities': total_entities,
            'data_source': 'who_icd_api_comprehensive',
            'data_type': 'medical_rag_full',
            'batch_count': len(batch_files)
        }
        
        # L∆∞u final chunks data trong th∆∞ m·ª•c chatbox/data
        current_dir = os.path.join(os.path.dirname(__file__), 'data')
        chunks_filepath = os.path.join(current_dir, "medical_chunks_with_metadata.pkl")
        index_filepath = os.path.join(current_dir, "medical_faiss_index.index")
        
        with open(chunks_filepath, "wb") as f:
            pickle.dump(final_chunks_data, f)
        print("‚úÖ ƒê√£ l∆∞u final chunks data v√†o medical_chunks_with_metadata.pkl")
        
        # T·∫°o v√† l∆∞u FAISS index
        print("üîÑ ƒêang t·∫°o FAISS index...")
        dimension = combined_embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(combined_embeddings)
        faiss.write_index(index, index_filepath)
        print("‚úÖ ƒê√£ l∆∞u FAISS index v√†o medical_faiss_index.index")
        
        print(f"\nüéâ HO√ÄN TH√ÄNH K·∫æT H·ª¢P!")
        print(f"üìä Th·ªëng k√™ cu·ªëi c√πng:")
        print(f"   - T·ªïng s·ªë entities: {total_entities}")
        print(f"   - T·ªïng s·ªë chunks: {len(all_chunks)}")
        print(f"   - K√≠ch th∆∞·ªõc embedding: {dimension}")
        print(f"   - S·ªë batch files: {len(batch_files)}")
        
        return {
            'success': True,
            'total_chunks': len(all_chunks),
            'total_entities': total_entities,
            'embedding_dimension': dimension,
            'batch_count': len(batch_files),
            'chunks_file': 'medical_chunks_with_metadata.pkl',
            'index_file': 'medical_faiss_index.index'
        }

def setup_icd_rag_system(client_id, client_secret, batch_size=100, combine_batches=True):
    """
    Thi·∫øt l·∫≠p RAG system v·ªõi TO√ÄN B·ªò d·ªØ li·ªáu ICD-11 (kh√¥ng gi·ªõi h·∫°n)
    
    Args:
        client_id: WHO ICD API client ID
        client_secret: WHO ICD API client secret  
        batch_size: S·ªë entities x·ª≠ l√Ω m·ªói batch (default: 100)
        combine_batches: C√≥ k·∫øt h·ª£p c√°c batch th√†nh file cu·ªëi kh√¥ng (default: True)
    """
    print("üè• B·∫Øt ƒë·∫ßu thi·∫øt l·∫≠p ICD RAG System...")
    
    # Kh·ªüi t·∫°o processor
    processor = ICDMmsDataProcessor(client_id, client_secret)
    
    # L·∫•y access token
    if not processor.get_access_token():
        print("‚ùå Kh√¥ng th·ªÉ l·∫•y access token. D·ª´ng setup.")
        return None
    
    # Kh·ªüi t·∫°o embedding model
    print("üîÑ ƒêang t·∫£i SentenceTransformer model...")
    try:
        embedder = SentenceTransformer('all-MiniLM-L6-v2')
        print("‚úÖ ƒê√£ t·∫£i embedding model th√†nh c√¥ng")
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i embedding model: {e}")
        return None
    
    # Kh·ªüi t·∫°o text splitter
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", ". ", ", ", " "]
    )
    
    # L·∫•y TO√ÄN B·ªò d·ªØ li·ªáu t·ª´ ICD API (kh√¥ng gi·ªõi h·∫°n)
    entities_data = processor.fetch_entities_for_rag(batch_size=batch_size)
    
    # D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω theo batch trong fetch_entities_for_rag
    print(f"‚úÖ ƒê√£ l·∫•y {len(entities_data)} entities v·ªõi batch processing")
    
    # Ki·ªÉm tra xem c√≤n d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω kh√¥ng
    seen_uris, _ = processor.load_existing_entities()
    
    # ƒê·∫øm children ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω
    unprocessed_count = 0
    for entity in entities_data:
        entity_data = entity.get('data', {})
        if 'child' in entity_data:
            for child_uri in entity_data['child']:
                if child_uri not in seen_uris:
                    unprocessed_count += 1
    
    if unprocessed_count > 0:
        print(f"üîÑ V·∫´n c√≤n {unprocessed_count} children ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω")
        print("üí° Ch·∫°y l·∫°i script ƒë·ªÉ ti·∫øp t·ª•c x·ª≠ l√Ω d·ªØ li·ªáu")
        return {
            'success': True, 
            'total_entities': len(entities_data), 
            'unprocessed_children': unprocessed_count,
            'message': f'C√≤n {unprocessed_count} children ch∆∞a x·ª≠ l√Ω. Ch·∫°y l·∫°i ƒë·ªÉ ti·∫øp t·ª•c.'
        }
    
    # K·∫øt h·ª£p t·∫•t c·∫£ batch files th√†nh file cu·ªëi c√πng (ch·ªâ khi kh√¥ng c√≤n d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω)
    if combine_batches:
        print("\nüîÑ B·∫Øt ƒë·∫ßu k·∫øt h·ª£p c√°c batch files...")
        result = processor.combine_all_batches()
        
        if result and result.get('success'):
            return result
        else:
            print("‚ùå L·ªói khi k·∫øt h·ª£p batch files")
            return None
    else:
        print("‚ö†Ô∏è Ch·ªâ t·∫°o batch files, kh√¥ng k·∫øt h·ª£p th√†nh file cu·ªëi")
        return {
            'success': True,
            'total_entities': len(entities_data),
            'message': 'Batch files created successfully. Run with combine_batches=True to create final files.'
        }

# Main execution
if __name__ == "__main__":
    # C·∫•u h√¨nh WHO ICD API
    CLIENT_ID = "4876fbe9-043e-417d-bbf6-e1e67ee1d749_1491c0e8-e6fe-4da6-be8c-8635d935a285"
    CLIENT_SECRET = "w6ilxOd6Ik/CXNdIjK0NmsNzc1krj6Ci/606KCWB2eM="
    
    print("üè• THI·∫æT L·∫¨P D·ªÆ LI·ªÜU CH·∫®N ƒêO√ÅN T·ª™ ICD-11 MMS LINEARIZATION")
    print("=" * 70)
    print("üìä S·ª≠ d·ª•ng: MMS (Mortality and Morbidity Statistics) Linearization")
    print("üîç Ch·ªâ l·∫•y d·ªØ li·ªáu quan tr·ªçng cho ch·∫©n ƒëo√°n b·ªánh")
    print("üì¶ D·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω theo batch 100 entities")
    print("üîÑ C√≥ th·ªÉ resume n·∫øu b·ªã gi√°n ƒëo·∫°n")
    print("=" * 70)
    
    # Thi·∫øt l·∫≠p RAG system v·ªõi to√†n b·ªô d·ªØ li·ªáu
    result = setup_icd_rag_system(CLIENT_ID, CLIENT_SECRET, batch_size=100, combine_batches=False)
    
    if result and result.get('success'):
        # Ch·ªâ test t√¨m ki·∫øm n·∫øu ƒë√£ combine batches ho·∫∑c kh√¥ng c√≤n d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω
        if not result.get('unprocessed_children', 0):
            print("\nüß™ Test t√¨m ki·∫øm RAG:")
            
            # Test search function
            try:
                # Th·ª≠ import relative tr∆∞·ªõc, fallback sang absolute import
                try:
                    from .medical_rag_utils import search_medical_symptoms_and_diseases
                except ImportError:
                    from medical_rag_utils import search_medical_symptoms_and_diseases
                
                test_queries = [
                    "ƒëau ƒë·∫ßu",
                    "s·ªët cao",
                    "kh√≥ th·ªü",
                    "ƒëau b·ª•ng",
                    "tim m·∫°ch"
                ]
                
                for query in test_queries:
                    print(f"\nüîç Test query: '{query}'")
                    results = search_medical_symptoms_and_diseases(query, top_k=3)
                    
                    if results:
                        for i, result in enumerate(results, 1):
                            metadata = result['metadata']
                            print(f"  {i}. {metadata.get('entity_name', 'Unknown')}")
                            print(f"     URI: {metadata.get('entity_uri', 'N/A')}")
                            print(f"     Relevance: {result['relevance_score']:.3f}")
                    else:
                        print("  Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£")
                        
            except Exception as e:
                print(f"‚ùå L·ªói khi test t√¨m ki·∫øm: {e}")
        else:
            print(f"\nüí° {result.get('message', 'C√≤n d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω')}")
    
    else:
        print("‚ùå Thi·∫øt l·∫≠p RAG system th·∫•t b·∫°i")
