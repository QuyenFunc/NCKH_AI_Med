#!/usr/bin/env python3
"""
AI Service Module - OpenRouter DeepSeek Integration
T√≠ch h·ª£p AI ƒë·ªÉ ph√¢n t√≠ch intent v√† t·∫°o response th√¥ng minh
"""
import json
import time
from typing import Dict, List, Any, Optional
from config import client

class MedicalAIService:
    """Medical AI Service using OpenRouter DeepSeek"""
    
    def __init__(self):
        self.client = client
        self.model = "deepseek/deepseek-chat-v3.1:free"  # Free model on OpenRouter
        self.max_tokens = 2000  # Reduce for faster response
        self.temperature = 0.7
        
        # Add caching to reduce API calls
        self.intent_cache = {}  # Cache intent classifications
        self.response_cache = {}  # Cache generated responses
        self.max_cache_size = 100
        
        self.stats = {
            'total_requests': 0,
            'intent_classifications': 0,
            'response_generations': 0,
            'avg_response_time': 0.0,
            'errors': 0,
            'cache_hits': 0
        }
    
    def classify_intent_with_ai(self, query: str, context: Dict = None) -> Dict[str, Any]:
        """S·ª≠ d·ª•ng AI ƒë·ªÉ ph√¢n lo·∫°i intent th√¥ng minh"""
        
        # Check cache first
        cache_key = f"intent_{hash(query)}"
        if cache_key in self.intent_cache:
            self.stats['cache_hits'] += 1
            cached_result = self.intent_cache[cache_key].copy()
            print(f"üíæ AI Intent from cache: {cached_result['intent']}")
            return cached_result
        
        start_time = time.time()
        self.stats['total_requests'] += 1
        self.stats['intent_classifications'] += 1
        
        # T·∫°o context cho AI
        context_str = ""
        if context and context.get('mentioned_entities'):
            entities = ', '.join(context['mentioned_entities'][-3:])  # Last 3 entities
            context_str = f"B·ªëi c·∫£nh cu·ªôc tr√¨nh: ƒë√£ ƒë·ªÅ c·∫≠p ƒë·∫øn {entities}. "
        
        system_prompt = """B·∫°n l√† chuy√™n gia y t·∫ø AI. Ph√¢n t√≠ch c√¢u h·ªèi y t·∫ø c·ªßa ng∆∞·ªùi d√πng v√† tr·∫£ v·ªÅ JSON v·ªõi:
1. intent: m·ªôt trong ["emergency", "symptom_analysis", "disease_inquiry", "treatment_advice", "prevention", "general_medical"]
2. confidence: ƒë·ªô tin c·∫≠y (0-1)
3. key_entities: danh s√°ch thu·∫≠t ng·ªØ y t·∫ø quan tr·ªçng
4. urgency_level: m·ª©c ƒë·ªô kh·∫©n c·∫•p ["low", "medium", "high", "critical"]
5. analysis: ph√¢n t√≠ch ng·∫Øn g·ªçn

Quy t·∫Øc ph√¢n lo·∫°i:
- emergency: tri·ªáu ch·ª©ng nguy hi·ªÉm, c·∫ßn c·∫•p c·ª©u ngay
- symptom_analysis: m√¥ t·∫£ tri·ªáu ch·ª©ng, mu·ªën hi·ªÉu nguy√™n nh√¢n
- disease_inquiry: h·ªèi v·ªÅ b·ªánh c·ª• th·ªÉ, th√¥ng tin b·ªánh l√Ω
- treatment_advice: h·ªèi c√°ch ƒëi·ªÅu tr·ªã, thu·ªëc, li·ªáu ph√°p
- prevention: h·ªèi c√°ch ph√≤ng ng·ª´a b·ªánh
- general_medical: c√¢u h·ªèi y t·∫ø chung

QUAN TR·ªåNG: Ch·ªâ tr·∫£ v·ªÅ JSON h·ª£p l·ªá, kh√¥ng c√≥ text th√™m."""

        user_prompt = f"""{context_str}C√¢u h·ªèi: "{query}"

Ph√¢n t√≠ch v√† tr·∫£ v·ªÅ JSON:"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=300,
                temperature=0.3  # Lower temperature for classification
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # Try to extract JSON from response
            try:
                # Look for JSON in the response
                if '{' in result_text and '}' in result_text:
                    start_idx = result_text.find('{')
                    end_idx = result_text.rfind('}') + 1
                    json_text = result_text[start_idx:end_idx]
                    result = json.loads(json_text)
                else:
                    raise ValueError("No JSON found in response")
            except (json.JSONDecodeError, ValueError) as e:
                print(f"‚ö†Ô∏è JSON parsing failed: {e}")
                print(f"Raw response: {result_text[:200]}...")
                raise Exception(f"Failed to parse AI response: {e}")
            
            # Validate and set defaults
            result['intent'] = result.get('intent', 'general_medical')
            result['confidence'] = min(max(result.get('confidence', 0.7), 0.0), 1.0)
            result['key_entities'] = result.get('key_entities', [])
            result['urgency_level'] = result.get('urgency_level', 'low')
            result['analysis'] = result.get('analysis', '')
            
            response_time = time.time() - start_time
            self._update_response_time(response_time)
            
            # Cache successful result
            if len(self.intent_cache) < self.max_cache_size:
                self.intent_cache[cache_key] = result.copy()
            
            print(f"ü§ñ AI Intent: {result['intent']} (confidence: {result['confidence']:.2f}) in {response_time:.2f}s")
            
            return result
            
        except Exception as e:
            print(f"‚ö†Ô∏è AI Intent classification failed: {e}")
            self.stats['errors'] += 1
            
            # Fallback to rule-based
            return self._fallback_intent_classification(query)
    
    def generate_medical_response(self, query: str, search_results: List[Dict], 
                                 intent_info: Dict, context: Dict = None, stream: bool = False):
        """S·ª≠ d·ª•ng AI ƒë·ªÉ t·∫°o response th√¥ng minh d·ª±a tr√™n search results"""
        
        start_time = time.time()
        self.stats['total_requests'] += 1
        self.stats['response_generations'] += 1
        
        # Chu·∫©n b·ªã context cho AI
        context_str = ""
        if context and context.get('mentioned_entities'):
            entities = ', '.join(context['mentioned_entities'][-3:])
            context_str = f"B·ªëi c·∫£nh cu·ªôc tr√≤ chuy·ªán: ƒë√£ th·∫£o lu·∫≠n v·ªÅ {entities}. "
        
        # Chu·∫©n b·ªã medical data
        medical_context = ""
        if search_results:
            medical_context = "Th√¥ng tin y t·∫ø t·ª´ c∆° s·ªü d·ªØ li·ªáu ICD-11:\n\n"
            for i, result in enumerate(search_results[:3], 1):
                entity_name = result.get('metadata', {}).get('entity_name', 'Unknown')
                text_snippet = result.get('text', '')[:300]
                medical_context += f"{i}. {entity_name}:\n{text_snippet}\n\n"
        
        # T·∫°o system prompt d·ª±a tr√™n intent
        intent = intent_info.get('intent', 'general_medical')
        urgency = intent_info.get('urgency_level', 'low')
        
        system_prompts = {
            'emergency': """B·∫°n l√† b√°c sƒ© c·∫•p c·ª©u AI. Ng∆∞·ªùi d√πng c√≥ t√¨nh hu·ªëng kh·∫©n c·∫•p. 
- ƒê√°nh gi√° m·ª©c ƒë·ªô nghi√™m tr·ªçng
- ƒê∆∞a ra h∆∞·ªõng d·∫´n c·∫•p c·ª©u ngay l·∫≠p t·ª©c
- Khuy·∫øn ngh·ªã g·ªçi 115 ho·∫∑c ƒë·∫øn b·ªánh vi·ªán
- S·ª≠ d·ª•ng tone nghi√™m t√∫c, kh·∫©n c·∫•p
- Kh√¥ng ch·∫©n ƒëo√°n ch√≠nh x√°c, ch·ªâ ƒë∆∞a ra h∆∞·ªõng d·∫´n an to√†n""",

            'symptom_analysis': """B·∫°n l√† b√°c sƒ© n·ªôi khoa AI chuy√™n ph√¢n t√≠ch tri·ªáu ch·ª©ng.
- Ph√¢n t√≠ch c√°c tri·ªáu ch·ª©ng ƒë∆∞·ª£c m√¥ t·∫£
- ƒê∆∞a ra c√°c kh·∫£ nƒÉng nguy√™n nh√¢n (kh√¥ng ch·∫©n ƒëo√°n ch·∫Øc ch·∫Øn)
- H·ªèi th√™m th√¥ng tin n·∫øu c·∫ßn
- Khuy·∫øn ngh·ªã thƒÉm kh√°m y t·∫ø n·∫øu c·∫ßn thi·∫øt
- S·ª≠ d·ª•ng tone chuy√™n nghi·ªáp, th√¢n thi·ªán""",

            'disease_inquiry': """B·∫°n l√† b√°c sƒ© chuy√™n khoa AI cung c·∫•p th√¥ng tin b·ªánh l√Ω.
- Gi·∫£i th√≠ch v·ªÅ b·ªánh m·ªôt c√°ch d·ªÖ hi·ªÉu
- ƒê∆∞a ra th√¥ng tin v·ªÅ nguy√™n nh√¢n, tri·ªáu ch·ª©ng, ƒëi·ªÅu tr·ªã
- S·ª≠ d·ª•ng th√¥ng tin y khoa ch√≠nh x√°c
- Khuy·∫øn ngh·ªã tham kh·∫£o chuy√™n gia
- S·ª≠ d·ª•ng tone gi√°o d·ª•c, chuy√™n nghi·ªáp""",

            'treatment_advice': """B·∫°n l√† d∆∞·ª£c sƒ©/b√°c sƒ© ƒëi·ªÅu tr·ªã AI.
- ƒê∆∞a ra th√¥ng tin v·ªÅ ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã
- Gi·∫£i th√≠ch v·ªÅ thu·ªëc, li·ªáu ph√°p (kh√¥ng k√™ ƒë∆°n c·ª• th·ªÉ)
- L∆∞u √Ω v·ªÅ t√°c d·ª•ng ph·ª•, ch·ªëng ch·ªâ ƒë·ªãnh
- Nh·∫•n m·∫°nh t·∫ßm quan tr·ªçng c·ªßa ch·ªâ ƒë·ªãnh y t·∫ø
- S·ª≠ d·ª•ng tone c·∫©n th·∫≠n, chuy√™n nghi·ªáp""",

            'prevention': """B·∫°n l√† b√°c sƒ© y t·∫ø d·ª± ph√≤ng AI.
- ƒê∆∞a ra l·ªùi khuy√™n ph√≤ng ng·ª´a b·ªánh
- Th·∫£o lu·∫≠n v·ªÅ l·ªëi s·ªëng l√†nh m·∫°nh
- ƒê·ªÅ xu·∫•t c√°c bi·ªán ph√°p d·ª± ph√≤ng c·ª• th·ªÉ
- Khuy·∫øn ngh·ªã t·∫ßm so√°t ƒë·ªãnh k·ª≥ n·∫øu c·∫ßn
- S·ª≠ d·ª•ng tone t√≠ch c·ª±c, khuy·∫øn kh√≠ch""",

            'general_medical': """B·∫°n l√† b√°c sƒ© gia ƒë√¨nh AI th√¢n thi·ªán.
- Tr·∫£ l·ªùi c√¢u h·ªèi y t·∫ø m·ªôt c√°ch d·ªÖ hi·ªÉu
- Cung c·∫•p th√¥ng tin s·ª©c kh·ªèe chung
- Khuy·∫øn kh√≠ch l·ªëi s·ªëng l√†nh m·∫°nh
- Lu√¥n nh·∫Øc nh·ªü tham kh·∫£o √Ω ki·∫øn chuy√™n gia
- S·ª≠ d·ª•ng tone th√¢n thi·ªán, d·ªÖ g·∫ßn"""
        }
        
        system_prompt = system_prompts.get(intent, system_prompts['general_medical'])
        
        # Th√™m h∆∞·ªõng d·∫´n chung
        system_prompt += """

QUAN TR·ªåNG:
- S·ª≠ d·ª•ng ti·∫øng Vi·ªát t·ª± nhi√™n, d·ªÖ hi·ªÉu
- Kh√¥ng ƒë∆∞a ra ch·∫©n ƒëo√°n ch√≠nh x√°c ho·∫∑c k√™ ƒë∆°n thu·ªëc c·ª• th·ªÉ
- Lu√¥n khuy·∫øn ngh·ªã tham kh·∫£o √Ω ki·∫øn b√°c sƒ© chuy√™n khoa
- S·ª≠ d·ª•ng th√¥ng tin y t·∫ø ƒë∆∞·ª£c cung c·∫•p l√†m cƒÉn c·ª©
- Tr·∫£ l·ªùi trong kho·∫£ng 200-400 t·ª´
- C√≥ th·ªÉ s·ª≠ d·ª•ng emoji ph√π h·ª£p ƒë·ªÉ th√¢n thi·ªán h∆°n"""

        user_prompt = f"""{context_str}

C√¢u h·ªèi c·ªßa b·ªánh nh√¢n: "{query}"

Intent ph√¢n t√≠ch: {intent} (m·ª©c ƒë·ªô kh·∫©n c·∫•p: {urgency})
C√°c th·ª±c th·ªÉ y t·∫ø quan tr·ªçng: {', '.join(intent_info.get('key_entities', []))}

{medical_context}

H√£y ƒë∆∞a ra l·ªùi khuy√™n y t·∫ø ph√π h·ª£p d·ª±a tr√™n th√¥ng tin tr√™n:"""

        try:
            if stream:
                # STREAMING MODE - Return generator
                stream_response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=self.max_tokens,
                    temperature=self.temperature,
                    stream=True  # Enable real streaming!
                )
                
                full_response = ""
                for chunk in stream_response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        yield {
                            'type': 'chunk',
                            'content': content,
                            'full_response': full_response,
                            'intent': intent,
                            'urgency_level': urgency
                        }
                
                # Final response data
                response_time = time.time() - start_time
                self._update_response_time(response_time)
                
                print(f"ü§ñ AI Streaming completed in {response_time:.2f}s")
                
                yield {
                    'type': 'final',
                    'response': full_response,
                    'ai_generated': True,
                    'response_time': response_time,
                    'intent': intent,
                    'urgency_level': urgency,
                    'model': self.model
                }
                
            else:
                # NON-STREAMING MODE - Original blocking behavior
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=self.max_tokens,
                    temperature=self.temperature
                )
                
                ai_response = response.choices[0].message.content.strip()
                response_time = time.time() - start_time
                self._update_response_time(response_time)
                
                print(f"ü§ñ AI Response generated in {response_time:.2f}s")
                
                return {
                    'response': ai_response,
                    'ai_generated': True,
                    'response_time': response_time,
                    'intent': intent,
                    'urgency_level': urgency,
                    'model': self.model
                }
            
        except Exception as e:
            print(f"‚ö†Ô∏è AI Response generation failed: {e}")
            self.stats['errors'] += 1
            
            # Fallback to template response
            fallback_result = self._fallback_response_generation(query, search_results, intent_info)
            
            if stream:
                yield {
                    'type': 'final',
                    'response': fallback_result['response'],
                    'ai_generated': False,
                    'response_time': fallback_result.get('response_time', 0),
                    'intent': intent,
                    'urgency_level': urgency,
                    'error': str(e),
                    'model': self.model
                }
            else:
                return fallback_result
    
    def _fallback_intent_classification(self, query: str) -> Dict[str, Any]:
        """Fallback rule-based intent classification"""
        query_lower = query.lower()
        
        # Emergency keywords
        if any(keyword in query_lower for keyword in ['c·∫•p c·ª©u', 'nguy hi·ªÉm', 'nghi√™m tr·ªçng', 'ƒëau ng·ª±c', 'kh√≥ th·ªü']):
            return {
                'intent': 'emergency',
                'confidence': 0.8,
                'key_entities': [],
                'urgency_level': 'high',
                'analysis': 'Rule-based emergency detection'
            }
        
        # Symptom analysis
        elif any(keyword in query_lower for keyword in ['ƒëau', 's·ªët', 'ho', 'bu·ªìn n√¥n', 'tri·ªáu ch·ª©ng']):
            return {
                'intent': 'symptom_analysis',
                'confidence': 0.7,
                'key_entities': [],
                'urgency_level': 'medium',
                'analysis': 'Rule-based symptom detection'
            }
        
        # Disease inquiry
        elif any(keyword in query_lower for keyword in ['b·ªánh', 'h·ªôi ch·ª©ng', 'r·ªëi lo·∫°n']):
            return {
                'intent': 'disease_inquiry',
                'confidence': 0.7,
                'key_entities': [],
                'urgency_level': 'low',
                'analysis': 'Rule-based disease inquiry'
            }
        
        # Treatment advice
        elif any(keyword in query_lower for keyword in ['ƒëi·ªÅu tr·ªã', 'thu·ªëc', 'ch·ªØa', 'c√°ch']):
            return {
                'intent': 'treatment_advice',
                'confidence': 0.6,
                'key_entities': [],
                'urgency_level': 'low',
                'analysis': 'Rule-based treatment inquiry'
            }
        
        else:
            return {
                'intent': 'general_medical',
                'confidence': 0.5,
                'key_entities': [],
                'urgency_level': 'low',
                'analysis': 'Rule-based general classification'
            }
    
    def _fallback_response_generation(self, query: str, search_results: List[Dict], 
                                    intent_info: Dict) -> Dict[str, Any]:
        """Fallback template-based response generation"""
        
        intent = intent_info.get('intent', 'general_medical')
        
        if search_results:
            context = f"D·ª±a tr√™n th√¥ng tin y t·∫ø t·ª´ ICD-11: {search_results[0].get('text', '')[:200]}..."
        else:
            context = "Kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ trong c∆° s·ªü d·ªØ li·ªáu."
        
        templates = {
            'emergency': f"‚ö†Ô∏è **T√åNH HU·ªêNG KH·∫®N C·∫§P** - N·∫øu ƒë√¢y l√† t√¨nh hu·ªëng nghi√™m tr·ªçng, h√£y g·ªçi 115 ngay. {context} Vui l√≤ng tham kh·∫£o √Ω ki·∫øn b√°c sƒ© c·∫•p c·ª©u.",
            'symptom_analysis': f"üîç **PH√ÇN T√çCH TRI·ªÜU CH·ª®NG** - {context} Khuy·∫øn ngh·ªã theo d√µi tri·ªáu ch·ª©ng v√† tham kh·∫£o √Ω ki·∫øn b√°c sƒ© n·∫øu c·∫ßn.",
            'disease_inquiry': f"üìö **TH√îNG TIN B·ªÜNH L√ù** - {context} ƒê√¢y l√† th√¥ng tin tham kh·∫£o, vui l√≤ng tham kh·∫£o chuy√™n gia y t·∫ø.",
            'treatment_advice': f"üíä **T∆Ø V·∫§N ƒêI·ªÄU TR·ªä** - {context} M·ªçi quy·∫øt ƒë·ªãnh ƒëi·ªÅu tr·ªã c·∫ßn ƒë∆∞·ª£c b√°c sƒ© ch·ªâ ƒë·ªãnh.",
            'general_medical': f"üè• **TH√îNG TIN Y T·∫æ** - {context} Vui l√≤ng tham kh·∫£o √Ω ki·∫øn chuy√™n gia y t·∫ø cho l·ªùi khuy√™n c·ª• th·ªÉ."
        }
        
        return {
            'response': templates.get(intent, templates['general_medical']),
            'ai_generated': False,
            'response_time': 0.1,
            'intent': intent,
            'urgency_level': intent_info.get('urgency_level', 'low'),
            'model': 'template_fallback'
        }
    
    def _update_response_time(self, response_time: float):
        """Update average response time statistics"""
        total_requests = self.stats['total_requests']
        current_avg = self.stats['avg_response_time']
        
        self.stats['avg_response_time'] = (
            (current_avg * (total_requests - 1) + response_time) / total_requests
        )
    
    def get_ai_stats(self) -> Dict[str, Any]:
        """Get AI service statistics"""
        return {
            'total_requests': self.stats['total_requests'],
            'intent_classifications': self.stats['intent_classifications'],
            'response_generations': self.stats['response_generations'],
            'avg_response_time': self.stats['avg_response_time'],
            'errors': self.stats['errors'],
            'error_rate': self.stats['errors'] / max(1, self.stats['total_requests']),
            'model': self.model,
            'status': 'active' if self.stats['errors'] < self.stats['total_requests'] * 0.5 else 'degraded'
        }

# Factory function
def get_medical_ai_service():
    """Get medical AI service instance"""
    return MedicalAIService()
