#!/usr/bin/env python3
"""
Enhanced Medical AI RAG Chatbot
TÃ­ch há»£p features tá»« app.py: logging, caching, query transformation, structured extraction
"""
from flask import Flask, request, jsonify, Response, stream_with_context
from flask_cors import CORS
from datetime import datetime
import uuid
import time
import re
import json
from collections import deque
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

def ensure_json_serializable(obj):
    """Convert any NumPy types to Python native types for JSON serialization"""
    if hasattr(obj, 'item'):  # NumPy scalar
        return obj.item()
    elif NUMPY_AVAILABLE and isinstance(obj, np.ndarray):
        return obj.tolist()
    elif NUMPY_AVAILABLE and isinstance(obj, (np.float32, np.float64)):
        return float(obj)
    elif NUMPY_AVAILABLE and isinstance(obj, (np.int32, np.int64)):
        return int(obj)
    elif isinstance(obj, dict):
        return {k: ensure_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [ensure_json_serializable(item) for item in obj]
    else:
        return obj

# Import RAG utilities
from medical_rag_utils import (
    search_medical_symptoms_and_diseases,
    create_medical_diagnostic_context,
    get_medical_statistics,
    improve_vietnamese_query
)

# Import Hybrid Search
from hybrid_search import get_hybrid_search_engine

# Import Enhanced Features tá»« app.py
try:
    from query_transformation import transform_medical_query
    QUERY_TRANSFORM_AVAILABLE = True
except ImportError:
    print("âš ï¸ Query transformation not available")
    QUERY_TRANSFORM_AVAILABLE = False

try:
    from structured_extraction import extract_medical_structure
    EXTRACTION_AVAILABLE = True
except ImportError:
    print("âš ï¸ Structured extraction not available")
    EXTRACTION_AVAILABLE = False

try:
    from medical_logging import log_query, log_search, log_llm, log_extraction, log_error, get_stats
    LOGGING_AVAILABLE = True
except ImportError:
    print("âš ï¸ Medical logging not available")
    LOGGING_AVAILABLE = False

# Import New Enhancement Modules
try:
    from enhanced_search_quality import get_search_quality_enhancer
    SEARCH_QUALITY_AVAILABLE = True
except ImportError:
    print("âš ï¸ Enhanced search quality not available")
    SEARCH_QUALITY_AVAILABLE = False

try:
    from medical_ner import get_medical_ner
    NER_AVAILABLE = True
except ImportError:
    print("âš ï¸ Medical NER not available")
    NER_AVAILABLE = False

try:
    from enhanced_confidence import get_confidence_calculator
    ENHANCED_CONFIDENCE_AVAILABLE = True
except ImportError:
    print("âš ï¸ Enhanced confidence not available")
    ENHANCED_CONFIDENCE_AVAILABLE = False

try:
    from performance_optimizer import get_performance_optimizer
    PERFORMANCE_OPTIMIZER_AVAILABLE = True
except ImportError:
    print("âš ï¸ Performance optimizer not available")
    PERFORMANCE_OPTIMIZER_AVAILABLE = False

# Import Ultra-Fast Optimization Modules
try:
    from optimized_hybrid_search import get_fast_hybrid_search_engine
    FAST_HYBRID_AVAILABLE = True
except ImportError:
    print("âš ï¸ Fast hybrid search not available")
    FAST_HYBRID_AVAILABLE = False

try:
    from advanced_cache import get_smart_query_cache, get_result_cache
    ADVANCED_CACHE_AVAILABLE = True
except ImportError:
    print("âš ï¸ Advanced cache not available")
    ADVANCED_CACHE_AVAILABLE = False

try:
    from async_processor import (
        get_parallel_entity_extractor, 
        get_parallel_search_processor,
        get_async_response_generator
    )
    ASYNC_PROCESSOR_AVAILABLE = True
except ImportError:
    print("âš ï¸ Async processor not available")
    ASYNC_PROCESSOR_AVAILABLE = False

try:
    from query_compression import get_query_compressor
    QUERY_COMPRESSION_AVAILABLE = True
except ImportError:
    print("âš ï¸ Query compression not available")
    QUERY_COMPRESSION_AVAILABLE = False

# Import AI Service for OpenRouter DeepSeek
try:
    from ai_service import get_medical_ai_service
    AI_SERVICE_AVAILABLE = True
except ImportError:
    print("âš ï¸ AI service not available")
    AI_SERVICE_AVAILABLE = False

app = Flask(__name__)
CORS(app)

class EnhancedMedicalChatbot:
    """Enhanced Medical AI RAG Chatbot vá»›i advanced features"""
    
    def __init__(self):
        self.conversation_history = {}
        self.session_timeout = 30 * 60  # 30 phÃºt
        self.hybrid_search_engine = None
        self.answer_cache = {}  # Cache for responses
        self.max_cache_size = 1000  # Giá»›i háº¡n cache
        self.stats = {
            'total_queries': 0,
            'cache_hits': 0,
            'search_times': [],
            'response_times': []
        }
        
        # Initialize enhancement modules
        self.search_quality_enhancer = None
        self.medical_ner = None
        self.confidence_calculator = None
        self.performance_optimizer = None
        
        # Initialize ultra-fast optimization modules
        self.fast_hybrid_engine = None
        self.smart_cache = None
        self.result_cache = None
        self.parallel_extractor = None
        self.parallel_search = None
        self.async_generator = None
        self.query_compressor = None
        
        # Initialize AI service
        self.ai_service = None
        
        # Ultra-fast performance tracking
        self.ultra_fast_stats = {
            'sub_3s_responses': 0,
            'optimization_wins': {
                'fast_hybrid': 0,
                'advanced_cache': 0,
                'async_processing': 0,
                'query_compression': 0
            }
        }
        
        self._init_all_systems()
    
    def _init_all_systems(self):
        """Initialize all enhancement systems"""
        print("ğŸš€ Initializing Enhanced Medical Chatbot Systems...")
        
        # Initialize hybrid search
        self._init_hybrid_search()
        
        # Initialize search quality enhancer
        if SEARCH_QUALITY_AVAILABLE:
            try:
                self.search_quality_enhancer = get_search_quality_enhancer()
                print("âœ… Search quality enhancer initialized")
            except Exception as e:
                print(f"âš ï¸ Search quality enhancer failed: {e}")
        
        # Initialize medical NER
        if NER_AVAILABLE:
            try:
                self.medical_ner = get_medical_ner()
                print("âœ… Medical NER initialized")
            except Exception as e:
                print(f"âš ï¸ Medical NER failed: {e}")
        
        # Initialize enhanced confidence calculator
        if ENHANCED_CONFIDENCE_AVAILABLE:
            try:
                self.confidence_calculator = get_confidence_calculator()
                print("âœ… Enhanced confidence calculator initialized")
            except Exception as e:
                print(f"âš ï¸ Enhanced confidence calculator failed: {e}")
        
        # Initialize performance optimizer
        if PERFORMANCE_OPTIMIZER_AVAILABLE:
            try:
                self.performance_optimizer = get_performance_optimizer()
                print("âœ… Performance optimizer initialized")
            except Exception as e:
                print(f"âš ï¸ Performance optimizer failed: {e}")
        
        # Initialize ultra-fast optimization systems
        self._init_ultra_fast_systems()
        
        print("ğŸ‰ All systems initialized!")
    
    def _init_ultra_fast_systems(self):
        """Initialize ultra-fast optimization systems"""
        print("âš¡ Initializing Ultra-Fast Optimization Systems...")
        
        # 1. Fast Hybrid Search
        if FAST_HYBRID_AVAILABLE:
            try:
                print("ğŸ”„ Loading fast hybrid search...")
                from medical_rag_utils import load_medical_data
                medical_data = load_medical_data()
                self.fast_hybrid_engine = get_fast_hybrid_search_engine(
                    chunks_data=medical_data,
                    medical_rag_utils=None
                )
                print("âœ… Fast hybrid search ready")
            except Exception as e:
                print(f"âš ï¸ Fast hybrid search failed: {e}")
        
        # 2. Advanced Caching
        if ADVANCED_CACHE_AVAILABLE:
            try:
                self.smart_cache = get_smart_query_cache()
                self.result_cache = get_result_cache()
                print("âœ… Advanced caching ready")
            except Exception as e:
                print(f"âš ï¸ Advanced caching failed: {e}")
        
        # 3. Async Processing
        if ASYNC_PROCESSOR_AVAILABLE:
            try:
                # Initialize with NER if available
                ner_models = []
                if self.medical_ner:
                    ner_models = [self.medical_ner]
                
                self.parallel_extractor = get_parallel_entity_extractor(ner_models)
                self.parallel_search = get_parallel_search_processor()
                self.async_generator = get_async_response_generator()
                print("âœ… Async processing ready")
            except Exception as e:
                print(f"âš ï¸ Async processing failed: {e}")
        
        # 4. Query Compression
        if QUERY_COMPRESSION_AVAILABLE:
            try:
                self.query_compressor = get_query_compressor()
                print("âœ… Query compression ready")
            except Exception as e:
                print(f"âš ï¸ Query compression failed: {e}")
        
        if AI_SERVICE_AVAILABLE:
            try:
                self.ai_service = get_medical_ai_service()
                print("âœ… AI Service (OpenRouter DeepSeek) ready")
            except Exception as e:
                print(f"âš ï¸ AI Service failed: {e}")
        
        print("ğŸš€ Ultra-fast optimizations ready!")
    
    def _init_hybrid_search(self):
        """Initialize hybrid search engine"""
        try:
            print("ğŸ”„ Initializing enhanced hybrid search engine...")
            self.hybrid_search_engine = get_hybrid_search_engine()
            print("âœ… Enhanced hybrid search engine initialized successfully")
            print(f"   ğŸ“Š BM25 corpus size: {len(self.hybrid_search_engine.tokenized_corpus) if self.hybrid_search_engine.tokenized_corpus else 0}")
            print(f"   ğŸ” Search method: Enhanced Hybrid (Semantic + BM25 + Caching)")
        except Exception as e:
            print(f"âš ï¸ Could not initialize hybrid search: {e}")
            print("   ğŸ”„ Falling back to pure semantic search")
            self.hybrid_search_engine = None

    def classify_medical_intent(self, query):
        """PhÃ¢n loáº¡i Ã½ Ä‘á»‹nh cá»§a cÃ¢u há»i y táº¿"""
        query_lower = query.lower()
        
        # Triá»‡u chá»©ng keywords
        symptom_keywords = [
            'Ä‘au', 'sá»‘t', 'ho', 'buá»“n nÃ´n', 'chÃ³ng máº·t', 'má»‡t má»i', 'khÃ³ thá»Ÿ',
            'Ä‘au Ä‘áº§u', 'Ä‘au bá»¥ng', 'tiÃªu cháº£y', 'tÃ¡o bÃ³n', 'phÃ¡t ban', 'ngá»©a',
            'sÆ°ng', 'viÃªm', 'nhiá»…m trÃ¹ng', 'cáº£m láº¡nh', 'cÃºm', 'bá»‡nh táº­t'
        ]
        
        # Emergency keywords  
        emergency_keywords = [
            'cáº¥p cá»©u', 'nguy hiá»ƒm', 'kháº©n cáº¥p', 'nghiÃªm trá»ng', 'náº·ng',
            'Ä‘au ngá»±c', 'khÃ³ thá»Ÿ', 'choÃ¡ng vÃ¡ng', 'báº¥t tá»‰nh', 'mÃ¡u'
        ]
        
        # Disease inquiry keywords
        disease_keywords = [
            'bá»‡nh', 'há»™i chá»©ng', 'rá»‘i loáº¡n', 'triá»‡u chá»©ng', 'nguyÃªn nhÃ¢n',
            'Ä‘iá»u trá»‹', 'thuá»‘c', 'phÃ²ng ngá»«a', 'biáº¿n chá»©ng'
        ]
        
        # Consultation keywords
        consultation_keywords = [
            'nÃªn lÃ m gÃ¬', 'cÃ¡ch Ä‘iá»u trá»‹', 'khÃ¡m bÃ¡c sÄ©', 'Ä‘i viá»‡n',
            'lá»i khuyÃªn', 'tÆ° váº¥n', 'hÆ°á»›ng dáº«n', 'chÄƒm sÃ³c'
        ]
        
        # Check patterns
        if any(keyword in query_lower for keyword in emergency_keywords):
            return 'emergency'
        elif any(keyword in query_lower for keyword in consultation_keywords):
            return 'medical_consultation'
        elif any(keyword in query_lower for keyword in disease_keywords):
            return 'disease_inquiry'
        elif any(keyword in query_lower for keyword in symptom_keywords):
            return 'symptom_analysis'
        else:
            return 'general_medical'

    def get_session(self, session_id):
        """Get or create conversation session"""
        current_time = datetime.now()
        
        if session_id not in self.conversation_history:
            self.conversation_history[session_id] = {
                'messages': [],
                'created_at': current_time,
                'last_activity': current_time
            }
        else:
            self.conversation_history[session_id]['last_activity'] = current_time
        
        return self.conversation_history[session_id]

    def clean_expired_sessions(self):
        """Clean expired conversation sessions"""
        current_time = datetime.now()
        expired_sessions = []
        
        for session_id, session in self.conversation_history.items():
            if (current_time - session['last_activity']).seconds > self.session_timeout:
                expired_sessions.append(session_id)
        
        for session_id in expired_sessions:
            del self.conversation_history[session_id]
            print(f"ğŸ—‘ï¸ Expired session cleaned: {session_id}")

    def extract_conversation_context(self, session):
        """Extract relevant context from conversation history"""
        if not session['messages']:
            return None
        
        # Get last few messages for context
        recent_messages = session['messages'][-6:]  # Last 3 exchanges
        
        context_info = {
            'mentioned_diseases': [],
            'mentioned_symptoms': [],
            'mentioned_treatments': [],
            'mentioned_entities': [],  # General medical entities
            'last_topic': None,
            'conversation_summary': []
        }
        
        # Extract entities from conversation
        for msg in recent_messages:
            content = msg.get('user_query', '') + ' ' + msg.get('response', '')
            content_lower = content.lower()
            
            # Extract diseases mentioned
            disease_keywords = ['tiá»ƒu Ä‘Æ°á»ng', 'cao huyáº¿t Ã¡p', 'viÃªm phá»•i', 'cÃºm', 'sá»‘t xuáº¥t huyáº¿t', 
                              'ung thÆ°', 'tim máº¡ch', 'Ä‘á»™t quá»µ', 'hen suyá»…n', 'dá»‹ á»©ng', 'viÃªm gan',
                              'viÃªm dáº¡ dÃ y', 'loÃ©t dáº¡ dÃ y', 'suy tháº­n', 'sá»i tháº­n', 'Covid-19']
            for disease in disease_keywords:
                if disease in content_lower and disease not in context_info['mentioned_diseases']:
                    context_info['mentioned_diseases'].append(disease)
                    context_info['mentioned_entities'].append(disease)
            
            # Extract symptoms mentioned  
            symptom_keywords = ['Ä‘au Ä‘áº§u', 'sá»‘t', 'ho', 'buá»“n nÃ´n', 'chÃ³ng máº·t', 'má»‡t má»i',
                               'khÃ³ thá»Ÿ', 'Ä‘au bá»¥ng', 'tiÃªu cháº£y', 'phÃ¡t ban', 'ngá»©a', 'Ä‘au ngá»±c',
                               'Ä‘au lÆ°ng', 'Ä‘au cá»•', 'khÃ n tiáº¿ng', 'ngháº¹t mÅ©i', 'cháº£y nÆ°á»›c mÅ©i']
            for symptom in symptom_keywords:
                if symptom in content_lower and symptom not in context_info['mentioned_symptoms']:
                    context_info['mentioned_symptoms'].append(symptom)
                    context_info['mentioned_entities'].append(symptom)
            
            # Extract treatments mentioned
            treatment_keywords = ['thuá»‘c', 'Ä‘iá»u trá»‹', 'pháº«u thuáº­t', 'khÃ¡m bÃ¡c sÄ©', 'nghá»‰ ngÆ¡i',
                                 'uá»‘ng nÆ°á»›c', 'Äƒn kiÃªng', 'táº­p thá»ƒ dá»¥c', 'vaccine', 'tiÃªm chá»§ng',
                                 'xÃ©t nghiá»‡m', 'chá»¥p X-quang', 'siÃªu Ã¢m', 'ná»™i soi']
            for treatment in treatment_keywords:
                if treatment in content_lower and treatment not in context_info['mentioned_treatments']:
                    context_info['mentioned_treatments'].append(treatment)
                    context_info['mentioned_entities'].append(treatment)
            
            # Add to conversation summary
            if msg.get('user_query'):
                context_info['conversation_summary'].append(f"User: {msg['user_query']}")
            if msg.get('intent'):
                context_info['last_topic'] = msg['intent']
        
        return context_info

    def resolve_query_references(self, query, context_info):
        """Resolve pronouns and references in query using context"""
        if not context_info:
            return query
        
        resolved_query = query
        query_lower = query.lower()
        
        # Common reference patterns vá»›i Vietnamese
        reference_patterns = {
            'cÃ¡i nÃ y': context_info['mentioned_entities'][-1:],
            'cÃ¡i Ä‘Ã³': context_info['mentioned_entities'][-1:],
            'cÃ¡i kia': context_info['mentioned_entities'][-2:-1] if len(context_info['mentioned_entities']) > 1 else context_info['mentioned_entities'][-1:],
            'nÃ³': context_info['mentioned_entities'][-1:],
            'chÃºng': context_info['mentioned_entities'][-2:],
            'bá»‡nh nÃ y': context_info['mentioned_diseases'][-1:],
            'bá»‡nh Ä‘Ã³': context_info['mentioned_diseases'][-1:],
            'bá»‡nh kia': context_info['mentioned_diseases'][-2:-1] if len(context_info['mentioned_diseases']) > 1 else context_info['mentioned_diseases'][-1:],
            'triá»‡u chá»©ng nÃ y': context_info['mentioned_symptoms'][-1:],
            'triá»‡u chá»©ng Ä‘Ã³': context_info['mentioned_symptoms'][-1:],
            'triá»‡u chá»©ng kia': context_info['mentioned_symptoms'][-2:-1] if len(context_info['mentioned_symptoms']) > 1 else context_info['mentioned_symptoms'][-1:],
            'thuá»‘c nÃ y': context_info['mentioned_treatments'][-1:],
            'thuá»‘c Ä‘Ã³': context_info['mentioned_treatments'][-1:],
            'cÃ¡ch Ä‘iá»u trá»‹ nÃ y': context_info['mentioned_treatments'][-1:],
            'phÆ°Æ¡ng phÃ¡p nÃ y': context_info['mentioned_treatments'][-1:]
        }
        
        # Replace references with actual entities
        for reference, entities in reference_patterns.items():
            if reference in query_lower and entities:
                # Replace with most recent relevant entity
                replacement = entities[0]
                resolved_query = resolved_query.replace(reference, replacement)
                print(f"ğŸ”— Resolved '{reference}' â†’ '{replacement}'")
        
        # Handle more complex reference patterns
        if any(pattern in query_lower for pattern in ['lÃ m sao', 'nhÆ° tháº¿ nÃ o', 'cÃ³ sao khÃ´ng', 'nguy hiá»ƒm khÃ´ng', 
                                                     'cáº§n lÃ m gÃ¬', 'cÃ³ nÃªn', 'cÃ³ thá»ƒ', 'pháº£i khÃ´ng']):
            context_added = False
            
            # Add disease context if available
            if context_info['mentioned_diseases'] and not context_added:
                disease_context = context_info['mentioned_diseases'][-1]
                resolved_query = f"{resolved_query} (vá» {disease_context})"
                print(f"ğŸ¯ Added disease context: {disease_context}")
                context_added = True
            
            # Add symptom context if no disease context
            elif context_info['mentioned_symptoms'] and not context_added:
                symptom_context = context_info['mentioned_symptoms'][-1]
                resolved_query = f"{resolved_query} (vá» {symptom_context})"
                print(f"ğŸ¯ Added symptom context: {symptom_context}")
                context_added = True
        
        return resolved_query
    
    def _create_sources_from_results(self, search_results):
        """Create sources list from search results"""
        sources = []
        for result in search_results:
            metadata = result.get('metadata', {})
            sources.append({
                'title': metadata.get('entity_name', 'Unknown'),
                'url': metadata.get('browser_url', ''),
                'relevance': result.get('relevance_score', result.get('semantic_score', 0)),
                'icd_code': metadata.get('entity_code', '')
            })
        return sources
    
    def _get_lightweight_session_context(self, session_id):
        """Get lightweight session context for caching"""
        if not session_id or session_id not in self.conversation_history:
            return {}
        
        session = self.conversation_history[session_id]
        return {
            'has_history': len(session.get('messages', [])) > 0,
            'recent_queries': len(session.get('messages', []))
        }

    def enhance_query_with_context(self, query, context_info):
        """Enhance query with conversation context for better search"""
        if not context_info:
            return query
        
        enhanced_query = query
        
        # Add relevant context entities to query for better search
        relevant_context = []
        
        # Add recent diseases for context (max 2)
        if context_info['mentioned_diseases']:
            relevant_context.extend(context_info['mentioned_diseases'][-2:])
        
        # Add recent symptoms for context (max 2)
        if context_info['mentioned_symptoms']:
            relevant_context.extend(context_info['mentioned_symptoms'][-2:])
        
        # Add recent treatments for context (max 1)
        if context_info['mentioned_treatments']:
            relevant_context.extend(context_info['mentioned_treatments'][-1:])
        
        if relevant_context:
            # Remove duplicates while preserving order
            unique_context = []
            for item in relevant_context:
                if item not in unique_context:
                    unique_context.append(item)
            
            context_string = ' '.join(unique_context[:3])  # Max 3 context items
            enhanced_query = f"{query} {context_string}"
            print(f"ğŸ” Enhanced query with context: {context_string}")
        
        return enhanced_query

    def create_medical_response(self, query, search_results, intent):
        """Create medical response based on intent and search results"""
        sources = []
        context_parts = []
        
        for i, result in enumerate(search_results, 1):
            metadata = result['metadata']
            entity_name = metadata.get('entity_name', 'Unknown')
            text_snippet = result['text'][:200] + "..." if len(result['text']) > 200 else result['text']
            
            context_parts.append(f"{i}. {entity_name}: {text_snippet}")
            
            sources.append({
                'title': entity_name,
                'url': metadata.get('browser_url', ''),
                'relevance': result.get('relevance_score', result.get('semantic_score', 0)),
                'icd_code': metadata.get('entity_code', '')
            })
        
        context = "\\n".join(context_parts)
        
        # Enhanced response templates based on intent
        if intent == 'emergency':
            response_template = f"""âš ï¸ **TÃŒNH HUá»NG Y Táº¾ KHáº¨N Cáº¤P**

ğŸš¨ **LÆ¯U Ã QUAN TRá»ŒNG:** Náº¿u Ä‘Ã¢y lÃ  tÃ¬nh huá»‘ng kháº©n cáº¥p, hÃ£y gá»i ngay:
- **115** (Cáº¥p cá»©u)
- **Äáº¿n bá»‡nh viá»‡n gáº§n nháº¥t**

**ThÃ´ng tin tham kháº£o tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u ICD-11:**
{context}

âš ï¸ **LÆ°u Ã½:** ÄÃ¢y chá»‰ lÃ  thÃ´ng tin tham kháº£o. KHÃ”NG trÃ¬ hoÃ£n viá»‡c tÃ¬m kiáº¿m sá»± chÄƒm sÃ³c y táº¿ kháº©n cáº¥p."""

        elif intent == 'symptom_analysis':
            response_template = f"""ğŸ” **PHÃ‚N TÃCH TRIá»†U CHá»¨NG**

**CÃ¢u há»i cá»§a báº¡n:** {query}

**ThÃ´ng tin tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u ICD-11:**
{context}

ğŸ’¡ **PhÃ¢n tÃ­ch:**
Dá»±a trÃªn cÃ¡c triá»‡u chá»©ng báº¡n mÃ´ táº£, cÃ³ thá»ƒ liÃªn quan Ä‘áº¿n cÃ¡c tÃ¬nh tráº¡ng sá»©c khá»e Ä‘Æ°á»£c liá»‡t kÃª á»Ÿ trÃªn. 

âš ï¸ **Khuyáº¿n nghá»‹:**
- Theo dÃµi triá»‡u chá»©ng trong 24-48 giá»
- Ghi chÃ©p thá»i gian, má»©c Ä‘á»™ nghiÃªm trá»ng
- Tham kháº£o Ã½ kiáº¿n bÃ¡c sÄ© náº¿u triá»‡u chá»©ng kÃ©o dÃ i hoáº·c náº·ng hÆ¡n
- KhÃ´ng tá»± cháº©n Ä‘oÃ¡n hoáº·c tá»± Ä‘iá»u trá»‹"""

        elif intent == 'disease_inquiry':
            response_template = f"""ğŸ“š **THÃ”NG TIN Bá»†NH LÃ**

**CÃ¢u há»i cá»§a báº¡n:** {query}

**ThÃ´ng tin chi tiáº¿t tá»« ICD-11:**
{context}

ğŸ“‹ **Tá»•ng quan:**
ThÃ´ng tin trÃªn cung cáº¥p kiáº¿n thá»©c y khoa chÃ­nh thá»©c tá»« Tá»• chá»©c Y táº¿ Tháº¿ giá»›i (WHO).

ğŸ’Š **LÆ°u Ã½ Ä‘iá»u trá»‹:**
- Má»i quyáº¿t Ä‘á»‹nh Ä‘iá»u trá»‹ cáº§n Ä‘Æ°á»£c bÃ¡c sÄ© chuyÃªn khoa quyáº¿t Ä‘á»‹nh
- KhÃ´ng tá»± Ã½ sá»­ dá»¥ng thuá»‘c mÃ  khÃ´ng cÃ³ chá»‰ Ä‘á»‹nh
- TuÃ¢n thá»§ theo dÃµi vÃ  tÃ¡i khÃ¡m Ä‘á»‹nh ká»³"""

        elif intent == 'medical_consultation':
            response_template = f"""ğŸ©º **TÆ¯ Váº¤N Y Táº¾**

**CÃ¢u há»i cá»§a báº¡n:** {query}

**ThÃ´ng tin tham kháº£o tá»« ICD-11:**
{context}

ğŸ’¬ **Lá»i khuyÃªn:**
Dá»±a trÃªn thÃ´ng tin y khoa hiá»‡n cÃ³, tÃ´i khuyáº¿n nghá»‹ báº¡n:

1. **Tham kháº£o Ã½ kiáº¿n chuyÃªn gia:** LiÃªn há»‡ vá»›i bÃ¡c sÄ© chuyÃªn khoa phÃ¹ há»£p
2. **Chuáº©n bá»‹ thÃ´ng tin:** Ghi chÃ©p triá»‡u chá»©ng, thá»i gian xuáº¥t hiá»‡n, cÃ¡c yáº¿u tá»‘ liÃªn quan
3. **Theo dÃµi sÃ¡t sao:** Ghi nháº­n má»i thay Ä‘á»•i vá» tÃ¬nh tráº¡ng sá»©c khá»e
4. **TuÃ¢n thá»§ hÆ°á»›ng dáº«n:** LÃ m theo chá»‰ Ä‘á»‹nh y táº¿ chÃ­nh thá»©c

ğŸ“ **Khi nÃ o cáº§n gáº¥p:** Náº¿u cÃ³ dáº¥u hiá»‡u nghiÃªm trá»ng, hÃ£y tÃ¬m kiáº¿m sá»± chÄƒm sÃ³c y táº¿ ngay láº­p tá»©c."""

        else:  # general_medical
            response_template = f"""ğŸ¥ **THÃ”NG TIN Y Táº¾ Tá»”NG QUÃT**

**CÃ¢u há»i cá»§a báº¡n:** {query}

**ThÃ´ng tin tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u ICD-11:**
{context}

ğŸ’¡ **ThÃ´ng tin tham kháº£o:**
ÄÃ¢y lÃ  thÃ´ng tin y khoa chÃ­nh thá»©c Ä‘Æ°á»£c cung cáº¥p tá»« há»‡ thá»‘ng phÃ¢n loáº¡i bá»‡nh quá»‘c táº¿ ICD-11 cá»§a WHO.

**Lá»i khuyÃªn:**
- Sá»­ dá»¥ng thÃ´ng tin nÃ y Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n vá» sá»©c khá»e
- LuÃ´n tham kháº£o Ã½ kiáº¿n bÃ¡c sÄ© cho cÃ¡c váº¥n Ä‘á» cá»¥ thá»ƒ
- KhÃ´ng tá»± cháº©n Ä‘oÃ¡n hoáº·c Ä‘iá»u trá»‹ dá»±a trÃªn thÃ´ng tin nÃ y"""
        
        # TÃ­nh confidence score
        avg_relevance = sum(result.get('relevance_score', result.get('semantic_score', 0)) for result in search_results[:3]) / min(3, len(search_results))
        confidence = min(avg_relevance * 1.2, 1.0)  # Scale up but cap at 1.0
        
        return {
            'response': response_template,
            'confidence': confidence,
            'sources': sources,
            'intent': intent
        }

    def semantic_search(self, query, session_id=None):
        """Ultra-fast enhanced chat function vá»›i streaming support"""
        if not session_id:
            session_id = str(uuid.uuid4())
        
        total_start_time = time.time()
        self.stats['total_queries'] += 1
        
        # 1. Ultra-fast cache check first (Advanced Cache)
        if self.smart_cache:
            try:
                session_context = self._get_lightweight_session_context(session_id)
                cached_result, cache_level = self.smart_cache.get_cached_result(
                    query, context=session_context
                )
                if cached_result:
                    print(f"ğŸ’¾ Using {cache_level} cached response")
                    self.stats['cache_hits'] += 1
                    self.ultra_fast_stats['optimization_wins']['advanced_cache'] += 1
                    cached_result['from_cache'] = True
                    cached_result['cache_level'] = cache_level
                    cached_result['total_time'] = time.time() - total_start_time
                    # âœ… Add missing fields for chat_stream compatibility
                    cached_result['session_id'] = session_id
                    session = self.get_session(session_id)
                    cached_result['context_info'] = self.extract_conversation_context(session)
                    return cached_result
            except Exception as e:
                print(f"âš ï¸ Advanced cache failed: {e}")
        
        # 2. Fallback to simple cache
        cache_key = f"{session_id}:{query}"
        if cache_key in self.answer_cache:
            print("ğŸ’¾ Using simple cached response")
            self.stats['cache_hits'] += 1
            cached_response = self.answer_cache[cache_key].copy()
            cached_response['from_cache'] = True
            cached_response['cache_level'] = 'simple'
            cached_response['total_time'] = time.time() - total_start_time
            # âœ… Add missing fields for chat_stream compatibility
            cached_response['session_id'] = session_id
            session = self.get_session(session_id)
            cached_response['context_info'] = self.extract_conversation_context(session)
            return cached_response
        
        # 3. Log initial query
        if LOGGING_AVAILABLE:
            try:
                log_query(query, session_id, "unknown")
            except Exception as e:
                print(f"âš ï¸ Logging failed: {e}")
        
        # 4. Query Compression (Ultra-fast optimization)
        compression_start = time.time()
        compressed_query_info = None
        if self.query_compressor:
            try:
                compressed_query_info = self.query_compressor.compress_query(query, target_length=40)
                if compressed_query_info.compression_ratio < 0.8:
                    print(f"ğŸ—œï¸ Query compressed: '{query}' â†’ '{compressed_query_info.compressed}'")
                    self.ultra_fast_stats['optimization_wins']['query_compression'] += 1
            except Exception as e:
                print(f"âš ï¸ Query compression failed: {e}")
        compression_time = time.time() - compression_start
        
        # 5. Clean expired sessions
        self.clean_expired_sessions()
        
        # 6. Get session
        session = self.get_session(session_id)
        
        # 7. Extract conversation context
        context_info = self.extract_conversation_context(session)
        
        # 8. Resolve references in query using context
        original_query = query
        resolved_query = self.resolve_query_references(query, context_info)
        
        # 9. Query transformation
        if QUERY_TRANSFORM_AVAILABLE:
            try:
                transformed_result = transform_medical_query(resolved_query)
                if isinstance(transformed_result, dict):
                    transformed_query = transformed_result.get('enriched_query', resolved_query)
                    if transformed_query != resolved_query:
                        print(f"ğŸ”„ Query transformed: '{resolved_query}' â†’ '{transformed_query}'")
                        resolved_query = transformed_query
                else:
                    if transformed_result != resolved_query:
                        print(f"ğŸ”„ Query transformed: '{resolved_query}' â†’ '{transformed_result}'")
                        resolved_query = transformed_result
            except Exception as e:
                print(f"âš ï¸ Query transformation failed: {e}")
        
        # 10. Enhance query with context for better search
        enhanced_query = self.enhance_query_with_context(resolved_query, context_info)
        
        # Use compressed query if available and beneficial
        if compressed_query_info and compressed_query_info.compression_ratio < 0.8:
            search_query = compressed_query_info.compressed
        else:
            search_query = enhanced_query
        
        # Log the query processing pipeline
        print(f"ğŸ’¬ Original: '{original_query}'")
        if resolved_query != original_query:
            print(f"ğŸ”— Resolved: '{resolved_query}'")
        if enhanced_query != resolved_query:
            print(f"ğŸ” Enhanced: '{enhanced_query}'")
        if compressed_query_info and search_query == compressed_query_info.compressed:
            print(f"ğŸ—œï¸ Using compressed: '{search_query}'")
        
        # Use resolved query for intent classification
        intent_query = resolved_query
        
        # 11. AI-powered intent classification
        if self.ai_service:
            try:
                intent_info = self.ai_service.classify_intent_with_ai(intent_query, context_info)
                intent = intent_info['intent']
                print(f"ğŸ¤– AI Intent: {intent} (confidence: {intent_info['confidence']:.2f})")
            except Exception as e:
                print(f"âš ï¸ AI intent classification failed: {e}")
                intent_info = {'intent': self.classify_medical_intent(intent_query), 'confidence': 0.5}
                intent = intent_info['intent']
        else:
            # Fallback to rule-based
            intent = self.classify_medical_intent(intent_query)
            intent_info = {'intent': intent, 'confidence': 0.5}
        
        # 12. Ultra-fast search with multiple strategies
        search_start_time = time.time()
        search_results = []
        search_method = "unknown"
        

        search_results = search_medical_symptoms_and_diseases(search_query, top_k=5)
        search_time = time.time() - search_start_time
        search_method = "fallback_semantic"
        print(f"ğŸ”„ Fallback semantic: {len(search_results)} results in {search_time:.2f}s")
        print(search_results)
        self.stats['search_times'].append(search_time)
        
        # 13. AI-powered response generation
        response_start_time = time.time()
        response_data = {
            "intent_info" : intent_info,
            "intent": intent,  # âœ… intent Ä‘Ã£ lÃ  string rá»“i, khÃ´ng cáº§n ['intent']
            "confidence": intent_info['confidence'],  # âœ… confidence náº±m trong intent_info
            "sources": search_results,
            "search_time": search_time,
            "total_time": time.time() - total_start_time,
            "session_id": session_id,
            "context_info": context_info
        }
        return response_data
    
    def _get_optimizations_used(self):
        """Get list of active optimizations"""
        optimizations = []
        
        if self.fast_hybrid_engine:
            optimizations.append('ultra_fast_hybrid_search')
        if self.smart_cache:
            optimizations.append('advanced_multi_level_caching')
        if self.parallel_extractor:
            optimizations.append('parallel_entity_extraction')
        if self.query_compressor:
            optimizations.append('intelligent_query_compression')
        if self.hybrid_search_engine:
            optimizations.append('standard_hybrid_search')
        if self.search_quality_enhancer:
            optimizations.append('enhanced_search_quality')
        if self.medical_ner:
            optimizations.append('medical_ner')
        if self.confidence_calculator:
            optimizations.append('enhanced_confidence_scoring')
        if self.ai_service:
            optimizations.append('ai_powered_intent_and_response')
        
        return optimizations
    
    def _get_performance_grade(self, response_time):
        """Get performance grade based on response time"""
        if response_time < 1.0:
            return 'A+ (Excellent < 1s)'
        elif response_time < 2.0:
            return 'A (Very Good < 2s)'
        elif response_time < 3.0:
            return 'B (Good < 3s)'
        elif response_time < 5.0:
            return 'C (Acceptable < 5s)'
        else:
            return 'D (Needs Improvement)'

    def get_chatbot_stats(self):
        """Get comprehensive ultra-fast chatbot statistics"""
        total_queries = self.stats['total_queries']
        avg_search_time = sum(self.stats['search_times']) / max(1, len(self.stats['search_times']))
        avg_response_time = sum(self.stats['response_times']) / max(1, len(self.stats['response_times']))
        
        stats = {
            # Basic stats
            'total_queries': total_queries,
            'cache_hits': self.stats['cache_hits'],
            'cache_hit_rate': self.stats['cache_hits'] / max(1, total_queries),
            'cache_size': len(self.answer_cache),
            'active_sessions': len(self.conversation_history),
            'avg_search_time': avg_search_time,
            'avg_response_time': avg_response_time,
            
            # Ultra-fast performance stats
            'ultra_fast_performance': {
                'sub_3s_responses': self.ultra_fast_stats['sub_3s_responses'],
                'sub_3s_success_rate': self.ultra_fast_stats['sub_3s_responses'] / max(1, total_queries),
                'avg_total_time': avg_search_time + avg_response_time,
                'performance_grade': self._get_performance_grade(avg_search_time + avg_response_time),
                'optimization_wins': self.ultra_fast_stats['optimization_wins']
            },
            
            # System availability
            'systems_available': {
                'fast_hybrid_search': bool(self.fast_hybrid_engine),
                'advanced_caching': bool(self.smart_cache),
                'async_processing': bool(self.parallel_extractor),
                'query_compression': bool(self.query_compressor),
                'standard_hybrid_search': bool(self.hybrid_search_engine),
                'enhanced_search_quality': bool(self.search_quality_enhancer),
                'medical_ner': bool(self.medical_ner),
                'enhanced_confidence': bool(self.confidence_calculator),
                'ai_service': bool(self.ai_service),
                'query_transformation': QUERY_TRANSFORM_AVAILABLE,
                'structured_extraction': EXTRACTION_AVAILABLE,
                'medical_logging': LOGGING_AVAILABLE
            },
            
            # Active optimizations
            'active_optimizations': self._get_optimizations_used(),
            
            # Legacy features for compatibility
            'features': [
                'Ultra-Fast Hybrid Search',
                'Advanced Multi-Level Caching',
                'Parallel Processing',
                'Query Compression',
                'Enhanced Search Quality',
                'Medical NER',
                'Enhanced Confidence Scoring',
                'AI-Powered Intent & Response (OpenRouter DeepSeek)',
                'Vietnamese Enhancement', 
                'Conversation Context',
                'Query Transformation' if QUERY_TRANSFORM_AVAILABLE else None,
                'Structured Extraction' if EXTRACTION_AVAILABLE else None,
                'Medical Logging' if LOGGING_AVAILABLE else None
            ]
        }
        
        # Remove None values from features
        stats['features'] = [f for f in stats['features'] if f is not None]
        
        return stats

# Initialize enhanced chatbot
chatbot = EnhancedMedicalChatbot()

@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'service': 'Enhanced Medical AI RAG Chatbot',
        'version': '2.0.0',
        'features': chatbot.get_chatbot_stats()['features']
    })

@app.route('/chat', methods=['POST'])
def chat():
    """Enhanced chat endpoint"""
    try:
        data = request.get_json()
        if not data or 'query' not in data:
            return jsonify({'error': 'Missing query parameter'}), 400
        
        query = data['query'].strip()
        if not query:
            return jsonify({'error': 'Empty query'}), 400
        
        session_id = data.get('session_id')
        
        # Get response from chatbot
        response = chatbot.chat(query, session_id)
        
        return jsonify(response)
        
    except Exception as e:
        print(f"âŒ Chat error: {e}")
        return jsonify({
            'error': 'Internal server error',
            'message': str(e)
        }), 500

@app.route('/chat/stream', methods=['POST'])
def chat_stream():
    """Streaming chat endpoint"""
    try:
        data = request.get_json()
        if not data or 'query' not in data:
            return jsonify({'error': 'Missing query parameter'}), 400
        
        query = data['query'].strip()
        session_id = data.get('session_id')
        
        # âœ… Fixed: Sá»­ dá»¥ng Ä‘Ãºng tÃªn method
        response = chatbot.semantic_search(query, session_id)
        intent_info = response['intent_info']
        intent = intent_info['intent']
        confidence = response['confidence']
        search_results = response['sources']
        search_time = response['search_time']
        total_time = response['total_time']
        session_id = response['session_id']
        context_info = response['context_info']
        
        # Generate streaming response
        def generate_stream():
            try:
                full_answer_parts = []  # âœ… Collect chunks to build full answer
                
                for chunk in chatbot.ai_service.generate_medical_response(
                    query, search_results, intent_info, context_info, stream=True
                ):
                    if chunk:
                        # âœ… Transform data to match frontend format
                        transformed_chunk = None
                        
                        if chunk.get('type') == 'chunk' and chunk.get('content'):
                            # Transform chunk data for frontend
                            full_answer_parts.append(chunk['content'])
                            transformed_chunk = {
                                'chunk': chunk['content'],  # âœ… 'content' â†’ 'chunk' 
                                'word_index': 0,
                                'session_id': session_id
                            }
                            # âœ… Ensure chunk data is JSON serializable
                            transformed_chunk = ensure_json_serializable(transformed_chunk)
                            
                        elif chunk.get('type') == 'final':
                            # Transform final data for frontend
                            if chunk.get('response'):
                                full_answer_parts.append(chunk['response'])
                            
                            # âœ… Transform sources format
                            transformed_sources = []
                            for source in search_results:
                                metadata = source.get('metadata', {})
                                transformed_sources.append({
                                    'title': metadata.get('entity_name', 'Unknown Medical Entity'),
                                    'url': metadata.get('browser_url', ''),
                                    'content': metadata.get('description', ''),
                                    'confidence': source.get('relevance_score', source.get('semantic_score', 0))
                                })
                            
                            transformed_chunk = {
                'type': 'final',
                                'confidence': confidence,
                                'sources': transformed_sources,
                                'processing_time': chunk.get('response_time', total_time),
                                'search_time': search_time,
                                'session_id': session_id,
                                'intent': intent
                            }
                            
                            # âœ… Ensure all data is JSON serializable
                            transformed_chunk = ensure_json_serializable(transformed_chunk)
                        
                        if transformed_chunk:
                            yield f"data: {json.dumps(transformed_chunk)}\n\n"
                
                # âœ… Build complete answer from collected parts
                full_answer = ''.join(full_answer_parts) if full_answer_parts else "AI response generated"
                
                # Save to conversation history  
                conversation_data = {
                    'timestamp': datetime.now().isoformat(),
                    'user_query': query,
                    'response': full_answer,
                    'confidence': confidence,
                    'sources_count': len(search_results),
                    'intent': intent,
                    'search_time': search_time,
                    'total_time': total_time
                }
                # âœ… Ensure conversation data is JSON serializable
                conversation_data = ensure_json_serializable(conversation_data)
                chatbot.conversation_history[session_id]['messages'].append(conversation_data)
                
                yield "data: [DONE]\n\n"
                
            except Exception as e:
                print(f"âŒ Stream generation error: {e}")
                # âœ… Send error in format frontend expects
                error_chunk = {
                    'type': 'error',
                    'error': str(e),
                    'session_id': session_id
                }
                # âœ… Ensure error data is JSON serializable
                error_chunk = ensure_json_serializable(error_chunk)
                yield f"data: {json.dumps(error_chunk)}\n\n"
        
        return Response(
            stream_with_context(generate_stream()),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'Connection': 'keep-alive',
                'X-Accel-Buffering': 'no',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Headers': 'Content-Type'
            }
        )
        
    except Exception as e:
        print(f"âŒ Stream error: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/medical_stats', methods=['GET'])
def medical_stats():
    """Enhanced medical statistics endpoint"""
    try:
        # Get base medical stats
        stats = get_medical_statistics()
        
        # Add enhanced chatbot stats
        chatbot_stats = chatbot.get_chatbot_stats()
        stats.update({
            'enhanced_chatbot': chatbot_stats,
            'model': 'all-MiniLM-L6-v2',
            'data_source': 'WHO ICD-11 API Enhanced'
        })
        
        return jsonify(stats)
        
    except Exception as e:
        return jsonify({
            'error': 'Failed to get medical statistics',
            'message': str(e)
        }), 500

@app.route('/session/<session_id>', methods=['GET'])
def get_session_info(session_id):
    """Get session information"""
    try:
        if session_id not in chatbot.conversation_history:
            return jsonify({'error': 'Session not found'}), 404
        
        session = chatbot.conversation_history[session_id]
        return jsonify({
            'session_id': session_id,
            'created_at': session['created_at'].isoformat(),
            'last_activity': session['last_activity'].isoformat(),
            'message_count': len(session['messages']),
            'messages': session['messages']
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/cache/stats', methods=['GET'])
def cache_stats():
    """Get cache statistics"""
    return jsonify({
        'cache_size': len(chatbot.answer_cache),
        'max_cache_size': chatbot.max_cache_size,
        'cache_hit_rate': chatbot.stats['cache_hits'] / max(1, chatbot.stats['total_queries']),
        'total_queries': chatbot.stats['total_queries'],
        'cache_hits': chatbot.stats['cache_hits']
    })

@app.route('/cache/clear', methods=['POST'])
def clear_cache():
    """Clear response cache"""
    chatbot.answer_cache.clear()
    if chatbot.smart_cache:
        chatbot.smart_cache.clear_cache()
    return jsonify({'message': 'All caches cleared successfully'})

@app.route('/ultra_fast_chat', methods=['POST'])
def ultra_fast_chat():
    """Ultra-fast chat endpoint with aggressive optimizations"""
    try:
        data = request.get_json()
        if not data or 'query' not in data:
            return jsonify({'error': 'Missing query parameter'}), 400
        
        query = data['query'].strip()
        if not query:
            return jsonify({'error': 'Empty query'}), 400
        
        session_id = data.get('session_id')
        
        # Use the same chat function but emphasize ultra-fast features
        response = chatbot.chat(query, session_id)
        
        # Add ultra-fast specific metadata
        response['endpoint'] = 'ultra_fast_chat'
        response['target_performance'] = '< 3 seconds'
        
        return jsonify(response)
        
    except Exception as e:
        print(f"âŒ Ultra-fast chat error: {e}")
        return jsonify({
            'error': 'Internal server error',
            'message': str(e)
        }), 500

@app.route('/performance_stats', methods=['GET'])
def performance_stats():
    """Get detailed performance statistics"""
    try:
        stats = chatbot.get_chatbot_stats()
        
        # Add additional performance insights
        performance_insights = []
        
        ultra_perf = stats['ultra_fast_performance']
        if ultra_perf['sub_3s_success_rate'] > 0.8:
            performance_insights.append("ğŸ† Excellent performance - Meeting sub-3s target")
        elif ultra_perf['sub_3s_success_rate'] > 0.6:
            performance_insights.append("âœ… Good performance - Close to target")
        else:
            performance_insights.append("âš ï¸ Performance needs improvement")
        
        if stats['cache_hit_rate'] > 0.3:
            performance_insights.append("ğŸ’¾ Cache working effectively")
        
        active_opts = len(stats['active_optimizations'])
        if active_opts >= 6:
            performance_insights.append(f"ğŸš€ All optimizations active ({active_opts})")
        elif active_opts >= 4:
            performance_insights.append(f"âš¡ Most optimizations active ({active_opts})")
        else:
            performance_insights.append(f"âš ï¸ Some optimizations missing ({active_opts})")
        
        stats['performance_insights'] = performance_insights
        
        return jsonify(stats)
        
    except Exception as e:
        return jsonify({
            'error': 'Failed to get performance statistics',
            'message': str(e)
        }), 500

@app.route('/ai_stats', methods=['GET'])
def ai_stats():
    """Get AI service statistics"""
    try:
        if not chatbot.ai_service:
            return jsonify({
                'error': 'AI service not available',
                'status': 'disabled'
            }), 404
        
        ai_stats = chatbot.ai_service.get_ai_stats()
        return jsonify(ai_stats)
        
    except Exception as e:
        return jsonify({
            'error': 'Failed to get AI statistics',
            'message': str(e)
        }), 500

if __name__ == '__main__':
    print("ğŸ¥ ULTRA-FAST ENHANCED MEDICAL AI RAG CHATBOT")
    print("=" * 80)
    print("ğŸ¤– Model: all-MiniLM-L6-v2 vá»›i Vietnamese enhancement")
    print("ğŸ” RAG: Ultra-Fast Multi-Strategy Search")
    print("ğŸ¯ Target: < 3 seconds response time")
    print()
    
    # Show system status
    stats = chatbot.get_chatbot_stats()
    systems = stats['systems_available']
    
    print("ğŸš€ OPTIMIZATION SYSTEMS:")
    print(f"   {'âœ…' if systems['fast_hybrid_search'] else 'âŒ'} Ultra-Fast Hybrid Search")
    print(f"   {'âœ…' if systems['advanced_caching'] else 'âŒ'} Advanced Multi-Level Caching")
    print(f"   {'âœ…' if systems['async_processing'] else 'âŒ'} Parallel Async Processing")
    print(f"   {'âœ…' if systems['query_compression'] else 'âŒ'} Intelligent Query Compression")
    print(f"   {'âœ…' if systems['enhanced_search_quality'] else 'âŒ'} Enhanced Search Quality")
    print(f"   {'âœ…' if systems['medical_ner'] else 'âŒ'} Medical NER")
    print(f"   {'âœ…' if systems['enhanced_confidence'] else 'âŒ'} Enhanced Confidence Scoring")
    print(f"   {'âœ…' if systems['ai_service'] else 'âŒ'} AI Service (OpenRouter DeepSeek)")
    print()
    
    print("ğŸ”¥ ACTIVE FEATURES:")
    for feature in stats['features']:
        print(f"   âœ… {feature}")
    print()
    
    active_opts = len(stats['active_optimizations'])
    total_opts = 8  # Total possible optimizations
    
    print(f"âš¡ PERFORMANCE STATUS:")
    print(f"   ğŸ¯ Target: < 3s response time")
    print(f"   ğŸš€ Optimizations: {active_opts}/{total_opts} active")
    print(f"   ğŸ’¾ Caching: {'Advanced + Simple' if systems['advanced_caching'] else 'Simple only'}")
    print(f"   ğŸ” Search: {'Ultra-Fast Multi-Strategy' if systems['fast_hybrid_search'] else 'Standard'}")
    print()
    
    print("ğŸ“¡ ENDPOINTS:")
    print("   ğŸ“¬ /chat - Standard enhanced chat")
    print("   ğŸš€ /ultra_fast_chat - Ultra-fast optimized chat")
    print("   ğŸ“Š /performance_stats - Detailed performance metrics")
    print("   ğŸ©º /medical_stats - Medical system statistics")
    print("   ğŸ¤– /ai_stats - AI service statistics")
    print()
    
    print("=" * 80)
    print("ğŸš€ Starting Ultra-Fast Enhanced Medical Chatbot Server...")
    print("ğŸ¯ Ready for sub-3s medical consultations!")
    
    app.run(debug=False, host='0.0.0.0', port=5001, threaded=True)  # Optimized for performance
